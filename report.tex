\documentclass[twoside]{article}
\usepackage{bmmost}
% % % % % % % % % % % % % % % гиперссылки
\definecolor{linkcolor}{HTML}{880088} % цвет ссылок
\definecolor{urlcolor}{HTML}{008888} 
\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}
%\pagecolor{cyan}

%\textwidth = 18cm
\oddsidemargin = -25pt
\evensidemargin = -25pt

\parindent=0mm
\begin{document}
	\title[Байесовская смесь распределений Бернулли]{Отчёт по практическому заданию №2. Байесовская смесь распределений Бернулли}
	\author{Ю.",Н.",Лукашкина}
	\email{julialukashkina@gmail.ru}
	\organization{МГУ имени М.",В.",Ломоносова, Москва}
	% \date{по умолчанию печатает дату трансляции, может содержать произвольный текст}
	\maketitle
	\begin{abstract}
		Данный документ содержит отчет по заданию БММО группы 417 "Байесовская смесь распределений Бернулли".
	\end{abstract}
	
	\tableofcontents
	
\section{Введение}\label{sec:quickstart}
	В данном задании было предложено вывести формулы для вариационного EM-алгоритма, реализовать данный ЕМ-алгоритм, посмотреть как изменяется кластеризация в зависимости от подаваемых параметров, произвести классификацию с помощью новых признаков, получаемых после работы EM-алгоритма.

\section{Вывод формул}
\subsection{Описание модели}
Пусть $\textbf{x} = (x_1, \dots, x_D)^\text{T}$ - набор из $D$ бинарных случайных величин $x_i$, каждая из которых имеет распределение
Бернулли с параметром $\mu_i$. Таким образом
\begin{equation*}\label{eq:i}
p(\textbf{x}\:|\:\boldsymbol{\mu}) = \prod_{i = 1}^D \mu_i^{x_i}(1-\mu_i)^{(1-x_i)},
\end{equation*}
где $\boldsymbol\mu = (\mu_1, \dots, \mu_D)^{\text{T}}$.
 Рассмотрим смесь таких распределений:
\begin{equation}\label{eq:pi}
p({\textbf{x}} \:|\: \boldsymbol{\mu}, \boldsymbol{\pi}) = \sum_{k = 1}^K {\pi}_k p({\textbf{x}} \:|\: {\mu}_k),	
\end{equation}


где $\boldsymbol{\mu} = \{\boldsymbol\mu_1,\: \dots,\: \boldsymbol\mu_K\}$, $\boldsymbol{\pi} =\{ \pi_1,\: \dots,\: \pi_K\}$.


Пусть задана обучающая выборка $\textbf{X} = \{\textbf{x}_1, \dots , \textbf{x}_N \}.$ Для каждого x введем скрытую переменную $\textbf{z} =
(z_1,\dots, z_K)^\text{T}$ — бинарный вектор, у которого только одна компонента равна 1, а все остальные равны 0. Тогда
можно записать условное распределение на $\textbf{x}$ при известной $\textbf{z}$:

\begin{equation*}\label{eq_2}
p(\textbf{x} \:|\: {\textbf{Z}},\: \boldsymbol{\mu}) = \prod_{k = 1}^K p({\textbf{x}} \:|\: \boldsymbol{\mu}_k)^{z_k} 
\end{equation*}
Введём распределение на ${\textbf{z}}$:
\begin{equation*}\label{eq_3}
p(\textbf{z} \:|\: \boldsymbol{\pi}) = \prod_{k = 1}^K {\pi}^{{z}_k}
\end{equation*}
Также введем априорные распределения на параметры $\boldsymbol{\mu}$ и $ \boldsymbol{\pi}$:


\begin{equation*}\label{eq:1}
p(\boldsymbol\pi \:|\: \boldsymbol\alpha) = \mathrm{Dir}(\boldsymbol{\pi} \:|\: \boldsymbol{\alpha}) = \cfrac{\Gamma(\sum\limits_{k = 1}^K \alpha_k)}{\prod_{k = 1}^K \Gamma(\alpha_k)} \prod\limits_{k = 1}^K {\pi}_k^{\alpha_k - 1}
= \left\{ \boldsymbol\alpha = (\alpha,\dots,\alpha)^\text{T}\right\} =
\cfrac{\Gamma(\textit{K}\alpha)}{\Gamma(\alpha)^{K}} \prod_{k = 1}^K {\pi}_k^{\alpha - 1}
\end{equation*}

\begin{equation*}\label{eq:2}
p(\boldsymbol{\mu}_k \:|\: a,\: b) = \prod_{i = 1}^D \mathrm{Beta}(\mu_{ki} \:|\: a,\: b) = \cfrac{\mu_{ki}^{a - 1} (1 - \mu_{ki})^{b - 1}}{\mathrm{B}(a,\: b)}
\end{equation*}

Совместное распределение модели:
\begin{equation*}\label{eq:3}
p(\textbf{X},\: \textbf{Z},\: \boldsymbol{\mu},\: \boldsymbol{\pi}\: |\: {\alpha},\: a,\: b) = 
p(\textbf{X} \:|\: \textbf{Z},\: \boldsymbol\mu)  p(\textbf{Z} \:|\: \boldsymbol{\pi}) \:p(\boldsymbol\pi \:|\: {\alpha})  \prod_{k = 1}^K \:p(\boldsymbol\mu_k \:| \: a,\: b)
\end{equation*}	

\begin{equation}\label{eq:sr}
p(\textbf{X},\: \textbf{Z},\: \boldsymbol{\mu},\: \boldsymbol{\pi}\: |\: {\alpha},\: a,\: b) = 
\prod_{n = 1}^N \prod_{k = 1}^K \left(\pi_{nk} \prod_{i = 1}^D \mu_{ki}^{x_{ni}} (1-\mu_{ki})^{1 - x_{ni}}\right)^{z_{nk}}
\mathrm{Dir}(\boldsymbol{\pi} \:|\: \boldsymbol{\alpha})
\prod_{k = 1}^K \prod_{i = 1}^D \mathrm{Beta}(\mu_{ki} \:|\: a,\: b) 
\end{equation}	



\subsection{E-шаг. Формулы для пересчета $q(\boldmath{\mu}), q(\textbf{Z})$}
Запишем логарифм совместного распределения: 
	\begin{align*}\label{eq:5}
	&\ln p(\textbf{X},\: \textbf{Z},\: \boldsymbol{\mu},\: \boldsymbol{\pi}\: |\: {\alpha},\: a,\: b) = \\
	&= \sum_{n = 1}^N \sum_{k = 1}^K z_{nk}\: \left[ \ln \pi_{nk} +  \sum_{i = 1}^D \Bigl( x_{ni}\: \ln\mu_{ki} + (1 - x_{ni})\ln(1-\mu_{ki})\Bigr)\right] + 
	\ln \mathrm{Dir}(\boldsymbol{\pi} \:|\: \boldsymbol{\alpha}) + 
	\sum_{k = 1}^K \sum_{i = 1}^D \ln\mathrm{Beta}(\mu_{ki} \:|\: a,\: b) =\\
	&= \sum_{n = 1}^N \sum_{k = 1}^K z_{nk}\:\left[\ln \pi_{nk} +  \sum_{i = 1}^D \Bigl( x_{ni}\: \ln\mu_{ki} + (1 - x_{ni})\ln(1-\mu_{ki})\Bigr)\right]
	+ \ln \Gamma(\textit{K}\alpha) - \textit{K}\ln\Gamma(\alpha) +   
	\sum_{k = 1}^K (\alpha - 1)\ln \pi_k + \\ 
	&+ \sum_{k = 1}^K \sum_{i = 1}^D \Bigl( (a - 1)\ln\mu_{ki} + (b - 1)\ln(1 - \mu_{ki})- \ln\mathrm{B}(a,\: b)\Bigr)
	\end{align*}
	На E-шаге вычисляется вариационное
	приближение:
	\[p({\textbf{Z}},\: \boldsymbol{\mu} \:|\: {\textbf{X}},\: \boldsymbol{\pi},\: \alpha,\: a,\: b) \approx q({\textbf{Z}})q(\boldsymbol{\mu}),\]
	Найдём $q(\boldsymbol\mu)$:
	\begin{align*}
	%\begin{multlined}
	\ln q(\boldsymbol\mu) &= \mathbb{E}_{q(\textbf{Z})} \ln p(\textbf{X},\: \textbf{Z},\: \boldsymbol\mu,\: \boldsymbol\pi \:|\: \boldsymbol\alpha,\: a,\: b) = \\
	&= \mathbb{E}_{q(\textbf{Z})}\sum_{n = 1}^N \sum_{k = 1}^K z_{nk}\: \sum_{i = 1}^D \Bigl( x_{ni}\: \ln\mu_{ki} + (1 - x_{ni})\ln(1-\mu_{ki})\Bigr)  
	+ \sum_{k = 1}^K \sum_{i = 1}^D \Bigl( (a - 1) \ln \mu_{ki} + (b - 1) \ln(1 - \mu_{ki}) \Bigr) + \mathrm{const} = \\
	&=\{\mathbb{E}[z_{nk}] = r_{nk}\} =\sum_{k = 1}^K \sum_{i = 1}^D \Biggl( 
	\sum_{n = 1}^N \Bigl[r_{nk} (x_{ni} \ln \mu_{ki} + 
	(1 - x_{ni}) \ln (1 - \mu_{ki}))\Bigr] 
	+ (a - 1) \ln \mu_{ki} + (b - 1) \ln (1 - \mu_{ki}) \Biggr) + \mathrm{const} = \\
	&= \sum_{k = 1}^K \sum_{i = 1}^D \Biggl( 
	\Big[ \sum_{n = 1}^N r_{nk} x_{ni} + a - 1 \Big] \ln \mu_{ki} + 
	\Big[ \sum_{n = 1}^N r_{nk} (1 - x_{ni}) + b - 1 \Big] \ln (1 - \mu_{ki}) \Biggl) + \:\mathrm{const}
	%\end{multlined}	
	\end{align*}
	
	\begin{align*}\label{eq_12}
	q(\boldsymbol{\mu}) = \prod_{k = 1}^K  \prod_{i = 1}^D \mathrm{Beta}(\mu_{ki} \:|\: \hat a_{ki},\: \hat b_{ki}) ,&&
	\hat a_{ki} = a + \sum_{n = 1}^N r_{nk} x_{ni}, \: \hat b_{ki} = b + \sum_{n = 1}^N r_{nk} (1 - x_{ni}),
	\end{align*}
	
	Запишем формулы для достаточных статистик:
	\begin{align*}
	\mathbb{E}_{q(\boldsymbol{\mu})}\ln q(\boldsymbol{\mu}) = \psi(\hat a) -
	\psi(\hat a+\hat b), && \mathbb{E}_{q(\boldsymbol{\mu})}\ln(1 - q(\boldsymbol{\mu}) )= \psi(\hat b) -	\psi(\hat a+\hat b).
	\end{align*}
	Формула для пересчёта  $\ln q(\textbf{Z})$:
	\begin{align*}
%	\begin{multlined}
	\ln q(\textbf{Z}) = \mathbb{E}_{q(\boldsymbol\mu)} \ln p(\textbf{X},\: \textbf{Z},\: \boldsymbol\mu,\: \boldsymbol\pi \:|\: \alpha,\: a,\: b) 
	&= \mathbb{E}_{q(\boldsymbol\mu)} \sum_{n = 1}^N  \sum_{k = 1}^K z_{nk}\: \left[ \ln\pi_{nk} +  \sum_{i = 1}^D \Bigl( x_{ni}\: \ln\mu_{ki} + (1 - x_{ni})\ln(1-\mu_{ki})\Bigr)\right] + \mathrm{const} = \\
	&= \sum_{n = 1}^N \sum_{k = 1}^K z_{nk} \underbrace{\left[\ln \pi_k +  \sum_{i = 1}^D \Bigl(x_{ni} \mathbb{E}_{q(\boldsymbol{\mu})} \ln\mu_{ki} + (1 - x_{ni}) \mathbb{E}_{q(\boldsymbol{\mu})} \ln(1 - \mu_{ki}) \Bigr)  \right]}_{\rho_{nk}} + \:\mathrm{const} 
%	\end{multlined}
	\end{align*}
	
	\begin{align*}
	q(\textbf{Z}) &= \prod_{n = 1}^N \prod_{k = 1}^K r_{nk}^{z_{nk}} \\
	r_{nk} &= \cfrac{\exp(\rho_{nk})}{\sum\limits_{j=1}^K \exp(\rho_{nj})}
	\end{align*}
	
	Заметим, что $\mathbb{E}[z_{nk}] = r_{nk}$.
	
\subsection{M-шаг. Формула для пересчёта $\pi$}
	На M-шаге вычисляется точечная оценка на $\pi$:

	\begin{align*}
	\mathbb{E}_{q(\textbf{Z})q(\boldsymbol{\mu})} \ln p(\textbf{X},\: \textbf{Z},\: \boldsymbol\mu,\: \boldsymbol{\pi}\: |\: \boldsymbol{\alpha},\: a,\: b) \rightarrow \underset{\boldsymbol{\pi}}{\mathrm{max}}
	\end{align*}
	Запишем Лагранжиан, учитывая, что вектор $\boldsymbol\pi$ --- вектор вероятностей ($\sum\limits_{k = 1}^K \pi_k = 1$).
	\begin{align*}
	\textbf{L} &=
	\sum_{n = 1}^N \sum_{k = 1}^K r_{nk} \left[\ln \pi_k +  \sum_{i = 1}^D \Bigl(x_{ni} \mathbb{E}_{q(\boldsymbol{\mu})} \ln\mu_{ki} + (1 - x_{ni}) \mathbb{E}_{q(\boldsymbol{\mu})} \ln(1 - \mu_{ki}) \Bigr)  \right] + 
	\ln \Gamma(K \alpha) - K \ln \Gamma(\alpha) +\\
	&+ \sum_{k = 1}^K (\alpha - 1)\ln \pi_k
	+ \sum_{k = 1}^K \sum_{i = 1}^D \Bigl[ (a - 1) \mathbb{E}_{q(\boldsymbol{\mu})}\ln \mu_{ki} + (b - 1) \mathbb{E}_{q(\boldsymbol{\mu})}\ln(1 - \mu_{ki}) - \ln \mathrm{B}(a, \:b) \Bigr]
	+ \lambda (\sum_{k = 1}^K \pi_k - 1)
	\end{align*}
	
	
	\begin{align*}
	\cfrac{d\textbf{L}}{d\pi_k} &= \sum_{n = 1}^N r_{nk}
	\cfrac{1}{\pi_k} + (\alpha - 1)\cfrac{1}{\pi_k} + \lambda = 0, &\longrightarrow & &
	\pi_k &= 
	\cfrac{\sum\limits_{n = 1}^N r_{nk} + (\alpha - 1)}{-\lambda},		\\
	-\lambda  \pi_k &= \sum\limits_{n = 1}^N r_{nk} + (\alpha - 1), &\longrightarrow &&
	-\sum_{k=1}^K\lambda \pi_k &= \sum_{k=1}^K \Bigl(\sum\limits_{n = 1}^N r_{nk} + (\alpha - 1)\Bigr),			\\
	-\lambda &= N + K(\alpha - 1), &\longrightarrow  &&
	\pi_k &= 
	\cfrac{\sum\limits_{n = 1}^N r_{nk} + (\alpha - 1)}{ N + K(\alpha - 1)}		\\
	\end{align*}
	
\subsection{Формула для подсчета вариационной нижней границы}
Выпишем формулу для вариационной нижней границы в общем виде: 
	\begin{align*}
	\mathcal{L} = \int q(\textbf{Z},\:\boldsymbol{\mu}) \ln \cfrac{p(\textbf{X},\: \textbf{Z},\: \boldsymbol\mu,\: \boldsymbol{\pi})}{q(\textbf{Z},\:\boldsymbol{\mu})} &=
	 \mathbb{E}_{q(\textbf{Z})q(\boldsymbol{\mu})}\Bigl[\ln p(\textbf{X},\: \textbf{Z},\: \boldsymbol\mu,\: \boldsymbol{\pi})\Bigr] - \mathbb{E}\Bigl[ \ln p(\textbf{Z},\:\boldsymbol{\mu})\Bigr] = \{p(\textbf{Z},\:\boldsymbol{\mu}) \approx q(\textbf{Z})q(\boldsymbol{\mu})\}  =\\ 
	 &= \mathbb{E}_{q(\textbf{Z})q(\boldsymbol{\mu})} \Bigl[\ln p(\textbf{X},\: \textbf{Z},\: \boldsymbol\mu,\: \boldsymbol{\pi})\Bigr] - \mathbb{E}_{q(\textbf{Z})}\Bigl[ \ln q(\textbf{Z})\Bigr] - \mathbb{E}_{q(\boldsymbol{\mu})} \Bigl[\ln q(\boldsymbol{\mu})\Bigr]
	\end{align*}
	\begin{align*}
		\mathcal{L} &=\\
		&= \sum_{n = 1}^N \sum_{k = 1}^K r_{nk}\:\left[\ln \pi_{nk} +  \sum_{i = 1}^D \Bigl( x_{ni}\: \mathbb{E}_{q(\boldsymbol{\mu})}\ln\mu_{ki} + (1 - x_{ni})\mathbb{E}_{q(\boldsymbol{\mu})}\ln(1-\mu_{ki})\Bigr)\right]
		+ \ln \Gamma(\textit{K}\alpha) - \textit{K}\ln\Gamma(\alpha) +   
		\sum_{k = 1}^K (\alpha - 1)\ln \pi_k + \\ 
		&+ \sum_{k = 1}^K \sum_{i = 1}^D \Bigl( (a - 1)\mathbb{E}_{q(\boldsymbol{\mu})}\ln\mu_{ki} + (b - 1)\mathbb{E}_{q(\boldsymbol{\mu})}\ln(1 - \mu_{ki})- \ln\mathrm{B}(a,\: b)\Bigr) -\\
		&- \sum_{n = 1}^N \sum_{k = 1}^K r_{nk} \ln r_{nk} 
		- \sum_{k = 1}^K \sum_{i = 1}^D \Bigl( (\hat a_{ki} - 1)\mathbb{E}_{q(\boldsymbol{\mu})}\ln\mu_{ki} + (\hat b_{ki} - 1)\mathbb{E}_{q(\boldsymbol{\mu})}\ln(1 - \mu_{ki})- \ln\mathrm{B}(\hat a_{ki},\: \hat b_{ki})\Bigr) 
	\end{align*}
	\section{Модельные данные}
	Протестируем полученный EM-алгорити на модельных данных. Сгенерируем выборку из заданного распределения (\ref{eq:sr}) с параметрами $a = b = 3,\: \alpha = 0.01,\: K = 70$. EM-алгоритм будем запускать на сетке значений 
	\[\ \boldsymbol{a} = [1, 2, 3], \: \: \boldsymbol{b} = [1, 2, 3], \: \: \boldsymbol\alpha = [0.01, 0.5, 0.1].\]
	Для того, чтобы оценить качество работы EM-алгоритма сравним априорные значения  $\boldsymbol{\pi}, \boldsymbol{\mu}$ c значениями, которые выдает EM-алгоритм ( $\boldsymbol{\pi_{EM}}, \boldsymbol{\mu_{EM}}$). Для сравнения будем использовать следующую метрику: 
	отсортируем вектора $\boldsymbol{\pi}, \boldsymbol{\pi_{EM}}$ по возрастанию и посчитаем модуль разности между ними. Отсортируем $\boldsymbol{\mu},  \boldsymbol{\mu_{EM}}$ по возрастанию $\boldsymbol{\pi}, \boldsymbol{\pi_{EM}}$ соответственно и посчитаем модуль среднего отклонения $\boldsymbol\mu_{ki}$ от $(\boldsymbol{\mu_{EM}})_{ki}$. На рис. \ref{fig:1} показаны графики зависимости ошибки на  $\boldsymbol{\pi}, \boldsymbol{\mu}$, в зависимости от $a,b$. Видно, что в целом ошибка небольшая, и минимум ошибки  $\boldsymbol{\pi}$ достигается при $a = b = 3$, значит EM-алгоритм правильно востановил данные.%(для построения графиков использовать скрипт modelDataFunction).
	
	\begin{figure}[h!]
		\begin{minipage}[h]{0.49\linewidth}
			\center{\includegraphics[width=1.05\linewidth]{pics/md1}} \\a) 
		\end{minipage}
		\hfill
		\begin{minipage}[h]{0.49\linewidth}
			\center{\includegraphics[width=1.05\linewidth]{pics/md2}} \\b)
		\end{minipage}
		\caption{a)$ \alpha = 0.01$, b) $\alpha = 0.05$.}
		\label{fig:1}
	\end{figure}
	
	\section{Данные MNIST}
	Протестируем полученный алгоритм на базе MNIST.
	Подготовленные бинаризованные данные MNIST были скачены	 по ссылке \href{https://yadi.sk/d/SSdrHQtWcZiEY}{https://yadi.sk/d/SSdrHQtWcZiEY}.
	Это csv файл с 60000 строк, в каждой строке записано бинарное изображение цифры от 0 до 9 и метка класса.
	Каждое изображение имеет размер 28 на 28 пикселей, соответственно в строке записано 785 чисел. (i, j) пиксель
	изображения записан в позиции 28i + j, нумерация с нуля. Последнее число в строке — метка класса.
\subsection{Функционал 	$\mathcal{L}(q)$}
Убедимся, что с увеличением колличества итераций функционал увеличивается. Запустим ЕМ-алгоритм с числом запусков из различных случайных начальных приближений равным 3 (см. рис. \ref{fig:8}). Действительно, функционал возрастает, при различных начальных значениях получаются различные конечные значения функционала.

\begin{figure}[h!]
	\begin{center}
		\begin{minipage}[h]{0.49\linewidth}
			\center{\includegraphics[width=1.05\linewidth]{pics/l}} 
		\end{minipage}
		\caption{Зависимость $\mathcal{L}(q)$ от числа итерацийё}
		\label{fig:8}
	\end{center}
\end{figure}
\subsection{Зависимость числа получаемых кластеров от априорного значения колличества компонент}
	Посмотрим зависимость $K$ (числа получаемых кластеров) от априорного значения колличества компонент, подаваемого в EM-алгоритм (options.K)  при фиксированных параметрах $\alpha, a, b$ (см. таблицу \ref{tab:1}).  Из таблицы видно, что при options.K $\le$ 50 полученное число кластеров примерно равно options.K. Это обусловлено тем, что при малых значениях априорного числа компонент ЕМ-алгоритм выделяет столько же кластеров, сколько и подавалось в алгоритм, т.к. в данных можно выделить достаточно много кластеров и никакие веса не зануляются. При options.K > 50 выделяется примерно одинаковое число кластеров (50 - 75), ЕМ-алгоритм сам сокращает априорное число колличества компонент до нужного оптимального значения. 
	\begin{table}[h]
		\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{}                & \textbf{\begin{tabular}[c]{@{}c@{}}a = b = 0.5\\ $\boldsymbol\alpha$ = 0.001\end{tabular}} \\ \hline
			\textbf{options.K = 5}  & 5   \\ \hline
			\textbf{options.K = 10} & 10    \\ \hline
			\textbf{options.K = 50} & 49   \\ \hline
			\textbf{options.K = 100}&  58    \\ \hline
			\textbf{options.K = 300}&  71    \\ \hline
			\textbf{options.K = 400} &  80       \\ \hline
			\textbf{options.K = 500} &  75       \\ \hline
		\end{tabular}
		\caption{Завиcимость колличества получаемых кластеров от начального значения $K$.}
		\label{tab:1}
	\end{center}
	\end{table} 

\subsection{Зависимость числа получаемых кластеров от $\alpha$}
Посмотрим зависимость $K$ (числа получаемых кластеров) от $\alpha$ (см. рис. \ref{fig:3}). Все графики при различных значениях $\alpha$ очень похожи, при малых значениях $a = b$ графики сливаются в один. Можно сделать вывод, что от параметра $\alpha$ количество выделенных кластеров не зависит. 

\begin{figure}[h!]
	\begin{center}
	\begin{minipage}[h]{0.49\linewidth}
		\center{\includegraphics[width=1.05\linewidth]{pics/mnistAlpha}} 
	\end{minipage}
	\caption{Зависимость $K$ от $\alpha$, $a = b$.}
	\label{fig:3}
	\end{center}
\end{figure}

\subsection{Зависимость числа получаемых кластеров от $a, b$}
При $a = b$ (см. рис. \ref{fig:4}) можно заметить, что с увеличением $a, b$ количество выделяемых кластеров уменьшается. При фиксированном $b = 1$, при малых значениях options.K в экспериментах  выделялось 49-50 кластеров, это было связано с тем, что данным при малых $a,\:b$ "не хватало" кластеров; при options.K = 500 (см. рис. \ref{fig:5}) количество выделяемых кластеров при увеличении $a$ тоже уменьшается.
\begin{figure}[h!]
	\begin{center}
		\begin{minipage}[h]{0.49\linewidth}
			\includegraphics[width=1.05\linewidth]{pics/k_a=b} 
			\caption{a) Зависимость $K$ от  $a = b$.}
			\label{fig:4}
		\end{minipage}
		\hfill
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1.05\linewidth]{pics/k_b=1}
			\caption{b) Зависимость $K$ от $a, b = 1$.}
			\label{fig:5}
		\end{minipage}
	\end{center}
\end{figure}

\subsection{Визуализиация полученных центров кластеров $\mathbb{E}_{q(\mu)}\mu$}
Визуализируем центры кластеров, полученных при различых параметрах алгоритма (см. рис \ref{fig:2}). На рисунке а) алгоритм выделил 48 кластеров, цифры чёткие. Можно заметить, что все начертания цифры нуль (и остальных цифр соответсвенно) отличаются друг от друга (поэтому много кластеров). На рисунке b) алгоритм выделил 6 кластеров, цифры расплывчатые, то есть алгоритм относит разные цифры к одному кластеру: описывает одним кластером много объектов --- из-за этого расплывчатость.

\begin{figure}[h!]
	\begin{center}		
	\begin{minipage}[h]{\linewidth}
		\center{\includegraphics[width=1.05\linewidth]{pics/mu_48}}  
		\caption{a)количество выявленных кластеров = 48}
	\end{minipage}
	\vfill
	\begin{minipage}[h]{0.48\linewidth}
		\center{\includegraphics[width=1.05\linewidth]{pics/mu_5_2}} 
		\caption{b)количество выявленных кластеров = 5}
	\end{minipage}
	\label{fig:2}
	\end{center}
\end{figure}

\section{Исследование логарифма правдоподобия}
Исследуем зависимость логарифма правдоподобия на обучающей и контрольной выборках от кластеризации. Правдоподобие вычисляется по формуле
\[p(\textbf{X}) = \prod_{n=1}^N p(\textbf{x}_n\: | \: \mathbb{E}_{q(\boldsymbol{\mu})} \boldsymbol{\mu},\boldsymbol{\pi}_{ML}), \]

где $p(\textbf{x}\:|\:\boldsymbol{\mu},\:\boldsymbol{\pi})$	 задано формулой \ref{eq:pi}, $\boldsymbol{\pi}_{ML}$ — точечная оценка на параметры $\boldsymbol\pi$, полученная в результате работы EM-алгоритма.
\[p(\textbf{X}) = \prod_{n=1}^N \sum_{k=1}^K (\pi_{ML})_k \Bigl( \prod_{i=1}^D\mathbb{E}_{q(\boldsymbol{\mu})} \boldsymbol{\mu}_{ki}^{x_{ni}} (1 - \mathbb{E}_{q(\boldsymbol{\mu})} \boldsymbol{\mu}_{ki})^{1 - x_{ni}}\Bigr) \]

\[\ln p(\textbf{X}) = \sum_{n=1}^N \ln \sum_{k=1}^K (\pi_{ML})_k \exp\Bigl( \sum_{i=1}^D {x_{ni}} \ln \mathbb{E}_{q(\boldsymbol{\mu})} \boldsymbol{\mu}_{ki}  + 	(1 - x_{ni}) \ln (1 - \mathbb{E}_{q(\boldsymbol{\mu})} \boldsymbol{\mu}_{ki})\Bigr) \]
Графики логарифма правдоподобия для тестовой и обучающей выборки очень похожи. На графике b) \ref{fig:6} при малых значениях $a$  происходит скачок вниз, он объясняется  тем, что выделяется слишком много кластеров.


\begin{figure}[h!]
	\begin{minipage}[h]{0.49\linewidth}
		\center{\includegraphics[width=1.05\linewidth]{pics/ll}} \\a) 
	\end{minipage}
	\hfill
	\begin{minipage}[h]{0.49\linewidth}
		\center{\includegraphics[width=1.05\linewidth]{pics/ll2}} \\b)
	\end{minipage}
	\caption{a)$ a = b $, b) $a, b = 1$.}
	\label{fig:6}
\end{figure}


\section{Классификация с новым признаком для $n_\text{ого}$ объекта: величины $q(z_{nk} = 1)$. Матрица точности.}


Рассмотрим величины $q(z_{nk} = 1)$ в качестве признаков n-го объекта. В качестве классификатора будем использовать svm из пакета libsvm. При различных параметрах $a, b, \alpha$ точность варируется в районе 80\%, что является хорошим результатом (с учетом того, что параметры $c, \gamma$ были установлены по умолчанию). Рассмотрим матрицу точности на контрольной выборке при $a = b = 1$ (см. таблицу \ref{table:1}). Все остальные параметры были установлены по умолчанию. Самое плохое качество  --- при классификации цифр 4 и 9, они часто "путаются" друг с другом. Также цифра 5 часто принимается за тройку, всё это объяснимо их похожими написаниями.  При $a = 0.01, b = 1$ ситуация практически ничем не отличается. При $a = b = 10$ (см. таблицу \ref{table:3}) ЕМ-алгоритмом было выделено меньше кластеров, поэтому значительно увеличились ошибки при классификации цифры 9 (она стала также ложно относиться ещё и к классу 7). Также увеличился процент троек, неверно классифицируемых как восьмерки. Данный результат был ожидаем, т.к. при уменьшении числа выделямых кластеров  больше цифр относиться к одному кластеру.   

Как уже было сказано ранее точность класификации при различных параметрах составила примерно 80\%, учитывая, что ЕМ-алгоритм значительно сократил размерность признакового пространства (с 784 признаков до 50), можно считать, что данный метод классификации является хорошим.


\begin{table}[h]
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multicolumn{1}{|l|}{} & \multicolumn{1}{c|}{\textbf{0}} & \multicolumn{1}{c|}{\textbf{1}} & \multicolumn{1}{c|}{\textbf{2}} & \multicolumn{1}{c|}{\textbf{3}} & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{\textbf{5}} & \multicolumn{1}{c|}{\textbf{6}} & \multicolumn{1}{c|}{\textbf{7}} & \multicolumn{1}{c|}{\textbf{8}} & \multicolumn{1}{c|}{\textbf{9}} \\ \hline
		\textbf{0} & \textbf{94.1473} & 0 & 0.5045 & 0.5045 & 0.1009 & 1.3118 & 1.1100 & 0 & 2.3209 & 0 \\ \hline
		\textbf{1} & 0 & \textbf{95.8647} & 0.4699 & 0.6579 & 0.0940 & 0.3759 & 0.5639 & 0 & 1.6917 & 0.2820 \\ \hline
		\textbf{2} & 0.5051 & 0.1010 & \textbf{93.9394} & 0.8081 & 0.5051 & 0.2020 & 0.1010 & 0.2020 & 3.6364 & 0 \\ \hline
		\textbf{3} & 0.3883 & 0.1942 & 1.1650 & \textbf{80.7767} & 0.1942 & 7.3786 & 0 & 0 & 8.4466 & 1.4563 \\ \hline
		\textbf{4} & 0.1017 & 0.5086 & 0.3052 & 0.2035 & \textbf{72.0244} & 0.3052 & 0.8138 & 1.6277 & 0.4069 & \textit{\textbf{23.7030}} \\ \hline
		\textbf{5} & 0.6557 & 0 & 0.4372 & \textit{\textbf{15.8470}} & 1.3115 & \textbf{70.9290} & 2.8415 & 0.1093 & 6.6667 & 1.2022 \\ \hline
		\textbf{6} & 0.7239 & 0.1034 & 0 & 0 & 0.2068 & 0.6205 & \textbf{97.8283} & 0 & 0.5171 & 0 \\ \hline
		\textbf{7} & 0.0917 & \multicolumn{1}{l|}{0.7339} & 1.7431 & 0.3670 & 2.8440 & 0.2752 & 0 & \textbf{80.8257} & 0.7339 & \textit{\textbf{12.3853}} \\ \hline
		\textbf{8} & 0.4955 & \multicolumn{1}{l|}{0.4955} & 0.3964 & 3.8652 & 0.4955 & 2.6759 & 0.2973 & 0.3964 & \textbf{90.2874} & 0.5946 \\ \hline
		\textbf{9} & 0.5203 & \multicolumn{1}{l|}{0} & 0.1041 & 1.6649 & \textit{\textbf{22.3725}} & 0.2081 & 0 & \textit{\textbf{13.2154}} & 1.7690 & \textbf{60.1457} \\ \hline
	\end{tabular}
	\label{table:1}
	\caption{a = b = 1, $\alpha$ = 0.001, accuracy = 83.88 \%}
	\end{center}	
\end{table}

\begin{table}[h]
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		& \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} \\ \hline
		\textbf{0} & \textbf{91.1201} & 0 & 2.3209 & 0.4036 & 0 & 1.2109 & 1.5136 & 0 & 3.4309 & 0 \\ \hline
		\textbf{1} & 0 & \textbf{95.3008} & 0.7519 & 0.5639 & 0.0940 & 0.1880 & 0.6579 & 0.1880 & 2.2556 & 0 \\ \hline
		\textbf{2} & 0.4040 & 0 & \textbf{94.8485} & 1.0101 & 0 & 0 & 0.6061 & 0 & 3.1313 & 0 \\ \hline
		\textbf{3} & 0.1942 & 0 & 1.9417 & \textbf{80.0000} & 0.0971 & 9.2233 & 0 & 0.4854 & 7.2816 & 0.7767 \\ \hline
		\textbf{4} & 0 & 0.4069 & 4.2726 & 0 & \textbf{65.2085} & 1.5259 & 0.5086 & 4.6796 & 0.4069 & \textit{\textbf{22.9908}} \\ \hline
		\textbf{5} & 0.5464 & 0 & 0.9836 & \textit{\textbf{15.7377}} & 1.2022 & \textbf{69.2896} & 2.8415 & 0.4372 & 8.0874 & 0.8743 \\ \hline
		\textbf{6} & 0.9307 & 0 & 2.0683 & 0 & 0 & 0.6205 & \textbf{95.9669} & 0 & 0.4137 & 0 \\ \hline
		\textbf{7} & 0.0917 & 0.5505 & 2.2936 & 0.2752 & 4.2202 & 0.1835 & 0.0917 & \textbf{75.9633} & 0.7339 & \textit{\textbf{15.5963}} \\ \hline
		\textbf{8} & 0.5946 & 0.2973 & 1.8831 & 5.5500 & 0.0991 & 2.7750 & 0.2973 & 0.1982 & \textbf{86.9177} & 1.3875 \\ \hline
		\textbf{9} & 0.3122 & 0.1041 & 0.4162 & 1.6649 & \textit{\textbf{20.3954}} & 0.2081 & 0 & \textit{\textbf{11.2383}} & 2.1852 & \textbf{63.4755} \\ \hline
	\end{tabular}
	\label{table:2}
	\caption{a = 0.01, b = 1, $\alpha$ = 0.001, accuracy = 81.98 \%}
	\end{center}	
\end{table}


\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			& \textbf{0} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} \\ \hline
			\textbf{0} & \textbf{92.7346} & 0 & 0.6054 & 0.1009 & 0.1009 & 1.4127 & 2.2200 & 0 & 2.8254 & 0 \\ \hline
			\textbf{1} & 0 & \textbf{95.9586} & 0.2820 & 0.0940 & 0 & 0.6579 & 0.4699 & 0.0940 & 2.3496 & 0.0940 \\ \hline
			\textbf{2} & 0.6061 & 0.3030 & \textbf{91.5152} & 0.1010 & 0.4040 & 0.2020 & 0.3030 & 0.3030 & 6.2626 & 0 \\ \hline
			\textbf{3} & 0.1942 & 0.1942 & 1.1650 & \textbf{61.8447} & 0.0971 & 9.1262 & 0.0971 & 0.0971 & \textit{\textbf{26.2136}} & 0.9709 \\ \hline
			\textbf{4} & 0.1017 & 0.5086 & 0.9156 & 0 & \textbf{55.9512} & 1.1190 & 0.8138 & 3.7640 & 0.9156 & \textit{\textbf{35.9105}} \\ \hline
			\textbf{5} & 1.0929 & 0 & 0.3279 & \textit{\textbf{18.6885}} & 1.3115 & \textbf{61.4208} & 2.7322 & 0.3279 & \textit{\textbf{13.0055}} & 1.0929 \\ \hline
			\textbf{6} & 0.6205 & 0.3102 & 0.1034 & 0 & 0 & 0.9307 & \textbf{97.4147} & 0 & 0.6205 & 0 \\ \hline
			\textbf{7} & 0 & 0.9174 & 1.3761 & 0 & 2.2936 & 0.2752 & 0 & \textbf{79.6330} & 1.1009 & \textit{\textbf{14.4037}} \\ \hline
			\textbf{8} & 0.5946 & 0.6938 & 0.6938 & 3.7661 & 0.4955 & 3.2706 & 0.1982 & 1.4866 & \textbf{88.3053} & 0.4955 \\ \hline
			\textbf{9} & 0.4162 & 0.3122 & 0.1041 & 1.2487 & \textit{\textbf{16.2331}} & 0.3122 & 0 & \textit{\textbf{16.8574}} & 1.8730 & \textbf{62.6431} \\ \hline
		\end{tabular}
		\label{table:3}
		\caption{a = b = 10, $\alpha$ = 0.001, accuracy = 78.98 \%}
	\end{center}	
\end{table}
\section{Выводы}
Работа ЕМ-алгоритма зависит от подаваемых параметров, однако в разной степени, например, от параметра $\alpha$  зависит не сильно, а от $a, b$ зависимость значительнее.

EM-алгоритм оказался хорошим методом сокращения размерности.



\end{document}
