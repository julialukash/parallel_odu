{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import artm\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths\n",
    "from print_helper import PrintHelper\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\modules\\helpers\\config_helper.py:19: DeprecationWarning: You passed a bytestring as `filenames`. This will not work on Python 3. Use `cp.read_file()` or switch to using Unicode strings across the board.\n",
      "  cfg.read(file_name)\n"
     ]
    }
   ],
   "source": [
    "config = ConfigPaths('config.cfg')\n",
    "plot_maker = PlotMaker()\n",
    "printer = PrintHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\UCI_filtered_ngramm_trimmed_without_names\\np_11_12_500\\models.txt Q:\\\\topic_modeling\\\\csi_science_collections.git\\..\\data\\postnauka\\UCI_collections\\UCI_filtered_ngramm_trimmed_without_names\n"
     ]
    }
   ],
   "source": [
    "print config.models_file_name, config.dataset_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_docs_lenths(_file_name):\n",
    "    with codecs.open(_file_name, 'r', 'utf-8') as fin:\n",
    "        # skip comment line\n",
    "        fin.readline()  \n",
    "        fin.readline()  \n",
    "        fin.readline()  \n",
    "        doc_len_counters = {}\n",
    "        for line in fin:\n",
    "            line_list = line.split(' ')\n",
    "            doc_idx = float(line_list[0])\n",
    "            val = float(line_list[2])\n",
    "            if not doc_len_counters.has_key(doc_idx):\n",
    "                doc_len_counters[doc_idx] = 0\n",
    "            doc_len_counters[doc_idx] += val\n",
    "    return doc_len_counters\n",
    "def save_pickle_file(dists, filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'wb')\n",
    "    pickle.dump(dists, pickle_file)\n",
    "    pickle_file.close()\n",
    "def load_pickle_file(filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'rb')\n",
    "    p_file = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    return p_file\n",
    "def save_model_pickle(_model_name, _model, _save=True):\n",
    "    phi = _model.get_phi()\n",
    "    phi = phi[(phi.T != 0).any()]\n",
    "    theta = _model.get_theta()    \n",
    "    saved_top_tokens = _model.score_tracker['top_tokens_score'].last_tokens\n",
    "    if _save:\n",
    "        save_pickle_file(phi, 'phi_{}.p'.format(_model_name))\n",
    "        save_pickle_file(theta, 'theta_{}.p'.format(_model_name))\n",
    "        save_pickle_file(saved_top_tokens, 'saved_top_tokens_{}.p'.format(_model_name))\n",
    "    return phi, theta, saved_top_tokens\n",
    "def load_model_pickle(_model_name, _distance_name):\n",
    "    phi = load_pickle_file('phi_{}.p'.format(_model_name))\n",
    "    theta = load_pickle_file('theta_{}.p'.format(_model_name))\n",
    "    saved_top_tokens = load_pickle_file('saved_top_tokens_{}.p'.format(_model_name))\n",
    "    distances = load_pickle_file('{}.p'.format(_distance_name))\n",
    "    return phi, theta, saved_top_tokens, distances\n",
    "def convert_to_vw(_nw, out_file):\n",
    "    with codecs.open(out_file, 'w', 'utf-8') as fout:\n",
    "        for idx, col in enumerate(_nw.columns):\n",
    "            if idx % 100 == 0:\n",
    "                print '[{}] processing column no {} of {}'.format(datetime.now(), idx, len(_nw.columns))\n",
    "            fout.write(u'doc_{} |@default_class '.format(col))\n",
    "            values = _nw[col]\n",
    "            for idx in values.index.values:\n",
    "                val = values[idx]\n",
    "                if val != 0:\n",
    "                    fout.write(u'{}:{} '.format(idx, val))\n",
    "            fout.write(u'\\n')\n",
    "def create_sample_dataset(phi, theta, doc_lenths):\n",
    "    ndw = phi.dot(theta)\n",
    "    index = 0\n",
    "    for col in ndw.columns:\n",
    "        if index % 100 == 0:\n",
    "            print '[{}] processing {} column'.format(datetime.now(), index)\n",
    "        index += 1\n",
    "        ndw[col] = ndw[col] * doc_lenths[col]\n",
    "    return ndw\n",
    "def create_sample_dataset_2(phi, theta, doc_lenths):\n",
    "    ndw = phi.dot(theta)\n",
    "    return ndw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_len_counters = get_docs_lenths(path.join(config.dataset_path, 'docword.pn.txt'))\n",
    "phi1, theta1, saved_top_tokens1, distances_hellinger_model1 = load_model_pickle('model1', 'distances_hellinger_model1')\n",
    "phi2, theta2, saved_top_tokens2, distances_hellinger_model2 = load_model_pickle('model2', 'distances_hellinger_model2')\n",
    "phi3, theta3, saved_top_tokens3, distances_hellinger_model3 = load_model_pickle('model3', 'distances_hellinger_model3')\n",
    "phi4, theta4, saved_top_tokens4, distances_hellinger_model4 = load_model_pickle('model4', 'distances_hellinger_model4')\n",
    "phi5, theta5, saved_top_tokens5, distances_hellinger_model5 = load_model_pickle('model5', 'distances_hellinger_model5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 11:31:19.102000] processing 0 column\n",
      "[2016-12-19 11:31:35.442000] processing 100 column\n",
      "[2016-12-19 11:31:50.497000] processing 200 column\n",
      "[2016-12-19 11:32:05.118000] processing 300 column\n",
      "[2016-12-19 11:32:19.218000] processing 400 column\n",
      "[2016-12-19 11:32:33.216000] processing 500 column\n",
      "[2016-12-19 11:32:47.018000] processing 600 column\n",
      "[2016-12-19 11:33:00.393000] processing 700 column\n",
      "[2016-12-19 11:33:12.783000] processing 800 column\n",
      "[2016-12-19 11:33:24.755000] processing 900 column\n",
      "[2016-12-19 11:33:36.280000] processing 1000 column\n",
      "[2016-12-19 11:33:47.446000] processing 1100 column\n",
      "[2016-12-19 11:33:58.080000] processing 1200 column\n",
      "[2016-12-19 11:34:08.280000] processing 1300 column\n",
      "[2016-12-19 11:34:18.018000] processing 1400 column\n",
      "[2016-12-19 11:34:27.299000] processing 1500 column\n",
      "[2016-12-19 11:34:36.096000] processing 1600 column\n",
      "[2016-12-19 11:34:44.507000] processing 1700 column\n",
      "[2016-12-19 11:34:52.400000] processing 1800 column\n",
      "[2016-12-19 11:34:59.876000] processing 1900 column\n",
      "[2016-12-19 11:35:06.875000] processing 2000 column\n",
      "[2016-12-19 11:35:13.640000] processing 2100 column\n",
      "[2016-12-19 11:35:20.114000] processing 2200 column\n",
      "[2016-12-19 11:35:26.032000] processing 2300 column\n",
      "[2016-12-19 11:35:31.487000] processing 2400 column\n",
      "[2016-12-19 11:35:36.518000] processing 2500 column\n",
      "[2016-12-19 11:35:40.804000] processing 2600 column\n",
      "[2016-12-19 11:35:44.611000] processing 2700 column\n",
      "[2016-12-19 11:35:47.952000] processing 2800 column\n",
      "[2016-12-19 11:35:50.845000] processing 2900 column\n",
      "[2016-12-19 11:35:53.278000] processing 3000 column\n",
      "[2016-12-19 11:35:55.241000] processing 3100 column\n",
      "[2016-12-19 11:35:56.813000] processing 3200 column\n",
      "[2016-12-19 11:35:57.857000] processing 3300 column\n",
      "[2016-12-19 11:35:58.450000] processing 3400 column\n",
      "[2016-12-19 11:35:58.752000] processing 0 column\n",
      "[2016-12-19 11:36:15.325000] processing 100 column\n",
      "[2016-12-19 11:36:30.359000] processing 200 column\n",
      "[2016-12-19 11:36:44.989000] processing 300 column\n",
      "[2016-12-19 11:36:59.168000] processing 400 column\n",
      "[2016-12-19 11:37:13.010000] processing 500 column\n",
      "[2016-12-19 11:37:26.335000] processing 600 column\n",
      "[2016-12-19 11:37:39.194000] processing 700 column\n",
      "[2016-12-19 11:37:52.134000] processing 800 column\n",
      "[2016-12-19 11:38:04.097000] processing 900 column\n",
      "[2016-12-19 11:38:15.580000] processing 1000 column\n",
      "[2016-12-19 11:38:26.772000] processing 1100 column\n",
      "[2016-12-19 11:38:37.383000] processing 1200 column\n",
      "[2016-12-19 11:38:47.769000] processing 1300 column\n",
      "[2016-12-19 11:38:57.500000] processing 1400 column\n",
      "[2016-12-19 11:39:06.813000] processing 1500 column\n",
      "[2016-12-19 11:39:15.659000] processing 1600 column\n",
      "[2016-12-19 11:39:24.088000] processing 1700 column\n",
      "[2016-12-19 11:39:32.011000] processing 1800 column\n",
      "[2016-12-19 11:39:39.534000] processing 1900 column\n",
      "[2016-12-19 11:39:46.613000] processing 2000 column\n",
      "[2016-12-19 11:39:53.165000] processing 2100 column\n",
      "[2016-12-19 11:39:59.265000] processing 2200 column\n",
      "[2016-12-19 11:40:05.215000] processing 2300 column\n",
      "[2016-12-19 11:40:10.381000] processing 2400 column\n",
      "[2016-12-19 11:40:15.115000] processing 2500 column\n",
      "[2016-12-19 11:40:19.388000] processing 2600 column\n",
      "[2016-12-19 11:40:23.259000] processing 2700 column\n",
      "[2016-12-19 11:40:26.618000] processing 2800 column\n",
      "[2016-12-19 11:40:29.499000] processing 2900 column\n",
      "[2016-12-19 11:40:32.075000] processing 3000 column\n",
      "[2016-12-19 11:40:34.039000] processing 3100 column\n",
      "[2016-12-19 11:40:35.546000] processing 3200 column\n",
      "[2016-12-19 11:40:36.594000] processing 3300 column\n",
      "[2016-12-19 11:40:37.190000] processing 3400 column\n"
     ]
    }
   ],
   "source": [
    "# ndw1 = create_sample_dataset(phi1, theta1, doc_len_counters)\n",
    "ndw2 = create_sample_dataset(phi2, theta2, doc_len_counters)\n",
    "# ndw3 = create_sample_dataset(phi3, theta3, doc_len_counters)\n",
    "# ndw4 = create_sample_dataset(phi4, theta4, doc_len_counters)\n",
    "# ndw5 = create_sample_dataset(phi5, theta5, doc_len_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 12:47:21.972000] processing 0 column\n"
     ]
    }
   ],
   "source": [
    "ndw2 = create_sample_dataset(phi2, theta2, doc_len_counters)\n",
    "ndw10 = create_sample_dataset_2(phi1, theta1, doc_len_counters)\n",
    "ndw20 = create_sample_dataset_2(phi2, theta2, doc_len_counters)\n",
    "ndw30 = create_sample_dataset_2(phi3, theta3, doc_len_counters)\n",
    "ndw40 = create_sample_dataset_2(phi4, theta4, doc_len_counters)\n",
    "ndw50 = create_sample_dataset_2(phi5, theta5, doc_len_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# можно посмотреть на какую-нибудь колонуку-документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 40\n",
    "col = ndw[ndw.columns[i]]\n",
    "print len(col[col > 0.15]), doc_len_counters[ndw.columns[i]] \n",
    "sorted(col, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndws = ndw.values.flatten()\n",
    "sns.distplot(ndws, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(ndws), len(ndws[ndws > 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-19 11:40:37.605000] processing column no 0 of 3446\n",
      "[2016-12-19 11:41:20.738000] processing column no 100 of 3446\n",
      "[2016-12-19 11:42:03.778000] processing column no 200 of 3446\n",
      "[2016-12-19 11:42:46.976000] processing column no 300 of 3446\n",
      "[2016-12-19 11:43:29.871000] processing column no 400 of 3446\n",
      "[2016-12-19 11:44:12.872000] processing column no 500 of 3446\n",
      "[2016-12-19 11:44:55.666000] processing column no 600 of 3446\n",
      "[2016-12-19 11:45:38.851000] processing column no 700 of 3446\n",
      "[2016-12-19 11:46:21.853000] processing column no 800 of 3446\n",
      "[2016-12-19 11:47:05.630000] processing column no 900 of 3446\n",
      "[2016-12-19 11:47:51.013000] processing column no 1000 of 3446\n",
      "[2016-12-19 11:48:36.099000] processing column no 1100 of 3446\n",
      "[2016-12-19 11:49:19.355000] processing column no 1200 of 3446\n",
      "[2016-12-19 11:50:02.578000] processing column no 1300 of 3446\n",
      "[2016-12-19 11:50:46.722000] processing column no 1400 of 3446\n",
      "[2016-12-19 11:51:29.595000] processing column no 1500 of 3446\n",
      "[2016-12-19 11:52:12.426000] processing column no 1600 of 3446\n",
      "[2016-12-19 11:52:55.813000] processing column no 1700 of 3446\n",
      "[2016-12-19 11:53:38.745000] processing column no 1800 of 3446\n",
      "[2016-12-19 11:54:21.591000] processing column no 1900 of 3446\n",
      "[2016-12-19 11:55:04.709000] processing column no 2000 of 3446\n",
      "[2016-12-19 11:55:47.528000] processing column no 2100 of 3446\n",
      "[2016-12-19 11:56:30.436000] processing column no 2200 of 3446\n",
      "[2016-12-19 11:57:13.403000] processing column no 2300 of 3446\n",
      "[2016-12-19 11:57:56.235000] processing column no 2400 of 3446\n",
      "[2016-12-19 11:58:38.955000] processing column no 2500 of 3446\n",
      "[2016-12-19 11:59:22.135000] processing column no 2600 of 3446\n",
      "[2016-12-19 12:00:05.080000] processing column no 2700 of 3446\n",
      "[2016-12-19 12:00:47.780000] processing column no 2800 of 3446\n",
      "[2016-12-19 12:01:30.773000] processing column no 2900 of 3446\n",
      "[2016-12-19 12:02:13.775000] processing column no 3000 of 3446\n",
      "[2016-12-19 12:02:56.751000] processing column no 3100 of 3446\n",
      "[2016-12-19 12:03:39.614000] processing column no 3200 of 3446\n",
      "[2016-12-19 12:04:22.880000] processing column no 3300 of 3446\n",
      "[2016-12-19 12:05:06.130000] processing column no 3400 of 3446\n",
      "[2016-12-19 12:05:27.392000] processing column no 0 of 3446\n",
      "[2016-12-19 12:06:16.232000] processing column no 100 of 3446\n",
      "[2016-12-19 12:07:05.342000] processing column no 200 of 3446\n",
      "[2016-12-19 12:07:48.732000] processing column no 300 of 3446\n",
      "[2016-12-19 12:08:31.460000] processing column no 400 of 3446\n",
      "[2016-12-19 12:09:14.535000] processing column no 500 of 3446\n",
      "[2016-12-19 12:09:57.386000] processing column no 600 of 3446\n",
      "[2016-12-19 12:10:40.089000] processing column no 700 of 3446\n",
      "[2016-12-19 12:11:23.029000] processing column no 800 of 3446\n",
      "[2016-12-19 12:12:05.778000] processing column no 900 of 3446\n",
      "[2016-12-19 12:12:48.725000] processing column no 1000 of 3446\n",
      "[2016-12-19 12:13:31.709000] processing column no 1100 of 3446\n",
      "[2016-12-19 12:14:14.827000] processing column no 1200 of 3446\n",
      "[2016-12-19 12:14:57.784000] processing column no 1300 of 3446\n",
      "[2016-12-19 12:15:40.717000] processing column no 1400 of 3446\n",
      "[2016-12-19 12:16:23.698000] processing column no 1500 of 3446\n",
      "[2016-12-19 12:17:06.497000] processing column no 1600 of 3446\n",
      "[2016-12-19 12:17:49.407000] processing column no 1700 of 3446\n",
      "[2016-12-19 12:18:32.459000] processing column no 1800 of 3446\n",
      "[2016-12-19 12:19:15.613000] processing column no 1900 of 3446\n",
      "[2016-12-19 12:19:58.186000] processing column no 2000 of 3446\n",
      "[2016-12-19 12:20:41.018000] processing column no 2100 of 3446\n",
      "[2016-12-19 12:21:23.879000] processing column no 2200 of 3446\n",
      "[2016-12-19 12:22:07.044000] processing column no 2300 of 3446\n",
      "[2016-12-19 12:22:49.879000] processing column no 2400 of 3446\n",
      "[2016-12-19 12:23:32.719000] processing column no 2500 of 3446\n",
      "[2016-12-19 12:24:15.858000] processing column no 2600 of 3446\n",
      "[2016-12-19 12:24:58.849000] processing column no 2700 of 3446\n",
      "[2016-12-19 12:25:44.075000] processing column no 2800 of 3446\n",
      "[2016-12-19 12:26:27.037000] processing column no 2900 of 3446\n",
      "[2016-12-19 12:27:09.926000] processing column no 3000 of 3446\n",
      "[2016-12-19 12:27:52.905000] processing column no 3100 of 3446\n",
      "[2016-12-19 12:31:18.166000] processing column no 3200 of 3446\n",
      "[2016-12-19 12:34:49.546000] processing column no 3300 of 3446\n",
      "[2016-12-19 12:42:18.854000] processing column no 3400 of 3446\n"
     ]
    }
   ],
   "source": [
    "output_vw_path = path.join(config.home_dir, '..\\\\data\\postnauka\\\\UCI_collections', 'sample_model_ndw')\n",
    "if not path.exists(output_vw_path):\n",
    "    mkdir(output_vw_path)\n",
    "# convert_to_vw(ndw1, path.join(output_vw_path, 'pn_model1.vw'))\n",
    "# convert_to_vw(ndw2, path.join(output_vw_path, 'pn_model2.vw'))\n",
    "convert_to_vw(ndw3, path.join(output_vw_path, 'pn_model3.vw'))\n",
    "convert_to_vw(ndw4, path.join(output_vw_path, 'pn_model4.vw'))\n",
    "# convert_to_vw(ndw5, path.join(output_vw_path, 'pn_model5.vw'))\n",
    "\n",
    "# output_vw_path = path.join(config.home_dir, '..\\\\data\\postnauka\\\\UCI_collections', 'sample_model_ndw_pure')\n",
    "# if not path.exists(output_vw_path):\n",
    "#     mkdir(output_vw_path)\n",
    "# convert_to_vw(ndw10, path.join(output_vw_path, 'pn_model1.vw'))\n",
    "# convert_to_vw(ndw20, path.join(output_vw_path, 'pn_model2.vw'))\n",
    "# convert_to_vw(ndw30, path.join(output_vw_path, 'pn_model3.vw'))\n",
    "# convert_to_vw(ndw40, path.join(output_vw_path, 'pn_model4.vw'))\n",
    "# convert_to_vw(ndw50, path.join(output_vw_path, 'pn_model5.vw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_to_vw(ndw2, path.join(output_vw_path, 'pn_model2.vw'))\n",
    "output_vw_path = path.join(config.home_dir, '..\\\\data\\postnauka\\\\UCI_collections', 'sample_model_ndw_pure')\n",
    "if not path.exists(output_vw_path):\n",
    "    mkdir(output_vw_path)\n",
    "convert_to_vw(ndw10, path.join(output_vw_path, 'pn_model1.vw'))\n",
    "convert_to_vw(ndw20, path.join(output_vw_path, 'pn_model2.vw'))\n",
    "convert_to_vw(ndw30, path.join(output_vw_path, 'pn_model3.vw'))\n",
    "convert_to_vw(ndw40, path.join(output_vw_path, 'pn_model4.vw'))\n",
    "convert_to_vw(ndw50, path.join(output_vw_path, 'pn_model5.vw'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
