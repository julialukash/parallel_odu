{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import artm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "\n",
    "import distances_helper as dh \n",
    "import print_helper as ph\n",
    "import create_model_helper as cmh\n",
    "import build_convex_hull_helper as bchh\n",
    "import different_models as dm\n",
    "\n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\pn_model3\\np_24_02_kl\\models.txt\n"
     ]
    }
   ],
   "source": [
    "config = ConfigPaths('config_sample_m3.cfg')\n",
    "print config.models_file_name\n",
    "models_file = open(config.models_file_name, 'a')\n",
    "\n",
    "plot_maker = PlotMaker()\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=config.output_batches_path,\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path=config.dictionary_path + '.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLOSEST_TOPICS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model_fn_1(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                             n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_20_m1_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_2(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                             n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_100_m2_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_3(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 300\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -20\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_20_m3_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_4(n_iteration):\n",
    "    tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "    tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                  tmp_model, _n_iterations=20, \n",
    "                                  _model_name='model_20_m4_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_1(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -3\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -3\n",
    "    tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_20_reg_1_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_2(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -5\n",
    "    tmp_model = fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20,                               \n",
    "                              _model_name='model_20_reg_2_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_3(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -10\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -10\n",
    "    tmp_model = fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20,             \n",
    "                              _model_name='model_20_reg_3_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_reg_4(n_iteration):\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100 + n_iteration,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = -5\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer'].tau = -10 \n",
    "    tmp_model = fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20,             \n",
    "                              _model_name='model_20_reg_4_iter_{}'.format(n_iteration))\n",
    "    return tmp_model\n",
    "\n",
    "def create_model_fn_20_complex_reg_1(n_iteration):\n",
    "    n_topics = 20\n",
    "    common_topics = [u'topic_0', u'topic_1']\n",
    "    subject_topics = list(set([u'topic_{}'.format(idx) for idx in range(2, 20)]) - set(common_topics))\n",
    "    tmp_model = create_model_complex(current_dictionary=dictionary, n_topics=n_topics, n_doc_passes=5, \n",
    "        seed_value=100 + n_iteration, n_top_tokens=15, p_mass_threshold=0.25, \n",
    "        common_topics=common_topics, subject_topics=subject_topics)\n",
    "    # subject topics\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer_subject', \n",
    "        topic_names=subject_topics))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer_subject', \n",
    "        topic_names=subject_topics, class_ids=['@default_class']))\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer_subject', \n",
    "        topic_names=subject_topics, class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer_subject'].tau = -0.5\n",
    "    tmp_model.regularizers['ss_phi_regularizer_subject'].tau = -0.5\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer_subject'].tau = -10\n",
    "\n",
    "    # common topics\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer_common', \n",
    "        topic_names=subject_topics))\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer_common', \n",
    "        topic_names=subject_topics, class_ids=['@default_class']))\n",
    "#     tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer_common', \n",
    "#         topic_names=subject_topics, class_ids=['@default_class']))\n",
    "    tmp_model.regularizers['ss_theta_regularizer_common'].tau = 0.5\n",
    "    tmp_model.regularizers['ss_phi_regularizer_common'].tau = 0.5\n",
    "#     tmp_model.regularizers['decorrelator_phi_regularizer_common'].tau = -10\n",
    "\n",
    "    tmp_model = fit_one_model_complex(plot_maker, batch_vectorizer, models_file, config, \n",
    "                                      tmp_model, _n_iterations=20,             \n",
    "                                      _model_name='model_20_complex_reg_1_iter_{}'.format(n_iteration))\n",
    "    return tmp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_pickle_file(dists, filename, _path=config.experiment_path):\n",
    "    pickle_filename = path.join(_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'wb')\n",
    "    pickle.dump(dists, pickle_file)\n",
    "    pickle_file.close()\n",
    "def save_model_pickle(_model_name, _model, _save=True):\n",
    "    phi = _model.get_phi()\n",
    "    phi = phi[(phi.T != 0).any()]\n",
    "    theta = _model.get_theta()    \n",
    "    saved_top_tokens = _model.score_tracker['top_tokens_score'].last_tokens\n",
    "    if _save:\n",
    "        save_pickle_file(phi, 'phi_{}.p'.format(_model_name))\n",
    "        save_pickle_file(theta, 'theta_{}.p'.format(_model_name))\n",
    "        save_pickle_file(saved_top_tokens, 'saved_top_tokens_{}.p'.format(_model_name))\n",
    "    return phi, theta, saved_top_tokens\n",
    "def load_pickle_file(filename, _path=config.experiment_path):\n",
    "    pickle_filename = path.join(_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'rb')\n",
    "    p_file = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    return p_file\n",
    "def load_model_pickle(_model_name, _distance_name=None, _path=config.experiment_path):\n",
    "    phi = load_pickle_file('phi_{}.p'.format(_model_name), _path)\n",
    "    theta = load_pickle_file('theta_{}.p'.format(_model_name), _path)\n",
    "    saved_top_tokens = load_pickle_file('saved_top_tokens_{}.p'.format(_model_name), _path)\n",
    "    distances = None\n",
    "    if _distance_name is not None:\n",
    "        distances = load_pickle_file('{}.p'.format(_distance_name), _path)\n",
    "    return phi, theta, saved_top_tokens, distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим оригинальный sample датасет (от model3), до этого скопировав в папку с batches нужные pickle файлы модели.\n",
    "Сначала провизуалируем по одной итерации каждой новой модели, а потом будем итерационно строить выпуклую оболочку для каждой модели по отдельности и затем сравнивать их. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2216, 100) (100, 3446)\n"
     ]
    }
   ],
   "source": [
    "phi_original, theta_original, saved_top_tokens_original, distances_hellinger_model_original = load_model_pickle('model3', 'distances_hellinger_model3', config.output_batches_path)\n",
    "print phi_original.shape, theta_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Запустить несколько раз с разным рандомом. Следить за тем, чтобы накапливались только независимые темы. Каждый раз смотреть. как проектируется на оригинальную матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now some info from run\n",
    "# distibution of topics in convex hull by iteration\n",
    "def plot_convex_hull_topics_iterations_distribution(_phi_convex_hull):\n",
    "    get_iteration_number_fn = lambda x: int(x[x.find('_', 6) + 1 : ])\n",
    "    phi_convex_hull_iteration_number = [get_iteration_number_fn(col) for col in _phi_convex_hull.columns]\n",
    "    phi_convex_hull_iteration_number = [(val, phi_convex_hull_iteration_number.count(val), 1.0 * phi_convex_hull_iteration_number.count(val) / len(phi_convex_hull_iteration_number)) for val in set(phi_convex_hull_iteration_number)]\n",
    "    print(phi_convex_hull_iteration_number)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    sns.barplot([x[0] for x in phi_convex_hull_iteration_number], [x[1] for x in phi_convex_hull_iteration_number], ax=ax1)\n",
    "    ax1.set_title('Number of topics from each iteration')\n",
    "    ax1.set_xlabel('n iteration')\n",
    "\n",
    "    sns.barplot([x[0] for x in phi_convex_hull_iteration_number], [x[2] for x in phi_convex_hull_iteration_number], ax=ax2)\n",
    "    ax2.set_title('Number of topics from each iteration (%)')\n",
    "    ax2.set_xlabel('n iteration')\n",
    "def plot_convex_hull_columns_change(iterations_info):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    ax1.plot([sum([val['n_topics_to_remove']for val in it['iterations_info_filter']]) for it in iterations_info], \n",
    "             color='r', label = 'total')\n",
    "    ax1.set_title('Num columns to remove')\n",
    "    get_topic_iteration_fn = lambda x: int(x[x.rfind('_') + 1 :])\n",
    "    get_topic_filter_iteration_list_fn = lambda x, y: [get_topic_iteration_fn(topic) for topic in x].count(y)\n",
    "    n_topics_removed_from_current_iteration = [sum([get_topic_filter_iteration_list_fn(val['removed_topics'], indx) for val in it['iterations_info_filter']]) for indx, it in enumerate(iterations_info)]\n",
    "    ax1.plot(n_topics_removed_from_current_iteration, color='b', label='current iteration')\n",
    "    ax1.set_xlabel('n iteration')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot([val['phi_convex_hull_shape'][1] for val in iterations_info], color='r')\n",
    "    ax2.set_title('Num columns of convex hull')\n",
    "    ax2.set_xlabel('n iteration')\n",
    "    ax2.legend()\n",
    "def plot_opt_res_fun(iterations_filtering_info_name):\n",
    "    %matplotlib inline\n",
    "    iterations_filtering_info = load_pickle_file(iterations_filtering_info_name)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    values = [x.fun for item in iterations_filtering_info for val in item for x in val['opt_res'].values()]\n",
    "    sns.distplot(values, color='r', bins=10, ax=ax1)\n",
    "    values = [[x.fun for val in item for x in val['opt_res'].values()] for item in iterations_filtering_info]\n",
    "    for val in values:\n",
    "        sns.distplot(val, bins=10, ax=ax2)\n",
    "def plot_opt_res_fun_filtering(iterations_filtering_info_name):\n",
    "    iterations_info = load_pickle_file(iterations_filtering_info_name)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n",
    "    values = [x.fun for item in iterations_info for val in item['iterations_info_filter'] for x in val['opt_res'].values()]\n",
    "    sns.distplot(values, color='r', bins=10, ax=ax1)\n",
    "    values = [[x.fun for val in item['iterations_info_filter'] for x in val['opt_res'].values()] for item in iterations_info]\n",
    "    for val in values:\n",
    "        sns.distplot(val, bins=10, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-02-24 12:44:07.177000] ********** iteration = 1 / 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d3e2ad81e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                      \u001b[0mn_closest_topics_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                      \u001b[0mopt_fun_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                      max_iteration=1, filtering_iteration=1)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplot_convex_hull_topics_iterations_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi_convex_hull_df_m2__0_6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;31m# plot_opt_res_fun('iterations_filtering_info_df_m2__0_6')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mQ:\\topic_modeling\\csi_science_collections.git\\modules\\helpers\\build_convex_hull_helper.pyc\u001b[0m in \u001b[0;36mbuild_convex_hull_delayed_filtering\u001b[0;34m(create_model_fn, get_topics_to_remove_fn, words, init_convex_hull, start_iteration, n_closest_topics_count, opt_fun_threshold, max_iteration, filtering_iteration)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[{}] ********** iteration = {} / {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmax_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[1;31m# build model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_phi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[1;31m# rename phi columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mQ:\\topic_modeling\\csi_science_collections.git\\modules\\helpers\\different_models.pyc\u001b[0m in \u001b[0;36mcreate_model_fn_2\u001b[0;34m(n_iteration)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_model_fn_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100 + n_iteration,\n\u001b[0m\u001b[1;32m     19\u001b[0m                              n_top_tokens=15, p_mass_threshold=0.25)\n\u001b[1;32m     20\u001b[0m     tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
      "\u001b[0;31mNameError\u001b[0m: global name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "OPT_FUN_THRESHOLD = 0.6\n",
    "phi_convex_hull_df_m2__0_6, iterations_info_df_m2__0_6, iterations_filtering_info_df_m2__0_6 = bchh \\\n",
    "                                                    .build_convex_hull_delayed_filtering(\n",
    "                                                     create_model_fn_2,\n",
    "                                                     bchh.get_topics_to_remove_by_opt_fun_and_distance, \n",
    "                                                     phi_original.index,\n",
    "                                                     init_convex_hull=[], start_iteration=0,\n",
    "                                                     n_closest_topics_count=2,\n",
    "                                                     opt_fun_threshold=0.6,\n",
    "                                                     max_iteration=1, filtering_iteration=1)\n",
    "plot_convex_hull_topics_iterations_distribution(phi_convex_hull_df_m2__0_6)\n",
    "# plot_opt_res_fun('iterations_filtering_info_df_m2__0_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_opt_x_close_to_th_count(opt_x, global_th=0.95):\n",
    "    cur_sum, sum_count = 0, 0\n",
    "    for val in sorted(opt_x)[::-1]:\n",
    "        if cur_sum + val <= global_th:\n",
    "            cur_sum += val\n",
    "            sum_count += 1\n",
    "        else:\n",
    "            break\n",
    "    return sum_count + 1, cur_sum\n",
    "def get_opt_x_granularity(opt):\n",
    "    return np.mean([get_opt_x_close_to_th_count(opt_res.x)[0] for topic, opt_res in opt.iteritems()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# permutations\n",
    "def test_permutation_optimization(_phi_original, dist_fn, n_closest_topics_count, _debug_print=False):\n",
    "    _phi_original = phi_original\n",
    "    n_topics = _phi_original.shape[1]\n",
    "    indices = np.random.choice(n_topics, n_topics, replace=False)\n",
    "    phi_permuted = _phi_original.iloc[:, indices]\n",
    "    distances = bchh.calculate_distances(dist_fn, phi_permuted, _phi_original)\n",
    "    opt_res_convex_hull = bchh.get_optimization_result(dh.hellinger_dist, None, phi_permuted, _phi_original,\n",
    "                                                             distances, n_closest_topics_count)\n",
    "    print get_opt_x_granularity(opt_res_convex_hull)\n",
    "    \n",
    "    if _debug_print:\n",
    "        for _, item in opt_res_convex_hull.items():\n",
    "             print item.optimized_column, item.fun, item.x[0:3], item.column_names[0:3]\n",
    "    matched_columns_count = 0\n",
    "    good_x_coefficient_count = 0\n",
    "    for _, item in opt_res_convex_hull.items():\n",
    "        matched_columns_count += item.optimized_column == item.column_names[0]\n",
    "        good_x_coefficient_count += item.x[0] > 0.99\n",
    "    matched_columns = 100.0 * matched_columns_count / n_topics\n",
    "    good_x_coefficient = 100.0 * good_x_coefficient_count / n_topics\n",
    "    print 'matched_columns = {}%, good_x_coefficient = {}%'.format(matched_columns, good_x_coefficient)\n",
    "    return matched_columns, good_x_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "matched_columns = 100.0%, good_x_coefficient = 100.0%\n"
     ]
    }
   ],
   "source": [
    "_,_ = test_permutation_optimization(phi_original, dh.hellinger_dist, n_closest_topics_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_combination_optimization(_phi_original, dist_fn, n_closest_topics_count, n_runs, _debug_print=False):\n",
    "    _eps = 0.15\n",
    "    not_found_count_list, matched_coeffs_count_list = [], []\n",
    "    for n_iter in range(n_runs):               \n",
    "        word_count, topics_count = phi_original.shape\n",
    "        combination_count = np.random.randint(2, 7)\n",
    "        combination_indices = np.random.choice(topics_count, combination_count, replace=False)\n",
    "        phi_combination = phi_original.iloc[:, combination_indices]\n",
    "        single_combination_weights = np.random.uniform(0, 1, combination_count)\n",
    "        single_combination_weights = single_combination_weights / sum(single_combination_weights)\n",
    "        combination_weights = np.array([single_combination_weights]*word_count)\n",
    "        combination_weights = pd.DataFrame(data=combination_weights, index=phi_combination.index, columns=phi_combination.columns)\n",
    "        phi_combination = phi_combination.multiply(combination_weights)\n",
    "        phi_combination = pd.DataFrame(phi_combination.sum(axis=1), index=phi_combination.index, columns=['combination_0'])\n",
    "\n",
    "        distances = bchh.calculate_distances(dh.hellinger_dist, phi_combination, _phi_original)\n",
    "        opt_res_convex_hull = bchh.get_optimization_result(dh.hellinger_dist, None, phi_combination, _phi_original,\n",
    "                                                                 distances, n_closest_topics_count)\n",
    "        results = []\n",
    "        for idx, col_name in enumerate(phi_original.columns[combination_indices]):\n",
    "            opt_cols_names = list(opt_res_convex_hull['combination_0'].column_names)\n",
    "            col_name_opt_idx = opt_cols_names.index(col_name) if col_name in opt_cols_names else -1\n",
    "            col_name_opt_x = opt_res_convex_hull['combination_0'].x[col_name_opt_idx] if col_name_opt_idx != -1 else 0\n",
    "            col_name_opt_idx = col_name_opt_idx if col_name_opt_idx != -1 else \\\n",
    "                               (0 if single_combination_weights[idx] < 0.1 else -1)\n",
    "            results.append((col_name, single_combination_weights[idx], col_name_opt_x))           \n",
    "        not_found_count = len([res for res in results if res[2] == -1]) * 100.0 / combination_count\n",
    "        matched_coeffs_count = 100.0 * len([res for res in results if abs(res[1] - res[2]) < _eps]) / combination_count\n",
    "        if _debug_print or not_found_count != 0 or matched_coeffs_count != 100: \n",
    "            print('it = {} / {} not_found_columns_count = {}%, matched_coeffs_count = {}%'.format(n_iter + 1, n_runs, \n",
    "                                                                            not_found_count, matched_coeffs_count))\n",
    "            for r in results:\n",
    "                print r\n",
    "        not_found_count_list.append(not_found_count)\n",
    "        matched_coeffs_count_list.append(matched_coeffs_count)\n",
    "    total_not_found_count = sum(not_found_count_list) / n_runs\n",
    "    total_matched_coeffs_count = sum(matched_coeffs_count_list) / n_runs\n",
    "    print('total not_found_columns_count = {}%, matched_coeffs_count = {}%'.format(total_not_found_count,\n",
    "                                                                                   total_matched_coeffs_count))\n",
    "    return total_not_found_count, total_matched_coeffs_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total not_found_columns_count = 0.0%, matched_coeffs_count = 100.0%\n"
     ]
    }
   ],
   "source": [
    "_,_ = test_combination_optimization(phi_original, dh.hellinger_dist, n_closest_topics_count=5, n_runs=100, _debug_print=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
