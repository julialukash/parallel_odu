{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import artm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "import distances_helper as dh \n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths\n",
    "from print_helper import PrintHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\UCI_filtered_ngramm_trimmed_without_names\\np_17_01\\models.txt\n"
     ]
    }
   ],
   "source": [
    "config = ConfigPaths('config.cfg')\n",
    "plot_maker = PlotMaker()\n",
    "printer = PrintHelper()\n",
    "print config.models_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_file = open(config.models_file_name, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(current_dictionary, n_topics, n_doc_passes, seed_value, n_top_tokens, p_mass_threshold):    \n",
    "    print '[{}] creating model'.format(datetime.now())\n",
    "    model = artm.ARTM(num_topics=n_topics, dictionary=current_dictionary, cache_theta=True, seed=seed_value, \n",
    "                  class_ids={'ngramm': 1.0, 'author_id': 0.0, 'author': 0.0, \n",
    "                             'post_tag': 0.0, 'projects': 0.0, 'category': 0.0,\n",
    "                             'following_users': 0.0})\n",
    "    model.num_document_passes = n_doc_passes\n",
    "    add_scores_to_model(model, n_top_tokens=n_top_tokens, p_mass_threshold=p_mass_threshold)\n",
    "    return model\n",
    "def add_scores_to_model(artm_model, n_top_tokens, p_mass_threshold):\n",
    "    print '[{}] adding scores'.format(datetime.now())\n",
    "    artm_model.scores.add(artm.PerplexityScore(name='perplexity_score',\n",
    "                                      dictionary=dictionary))\n",
    "    artm_model.scores.add(artm.SparsityPhiScore(name='ss_phi_score', class_id='ngramm'))\n",
    "    artm_model.scores.add(artm.SparsityThetaScore(name='ss_theta_score'))\n",
    "    artm_model.scores.add(artm.TopicKernelScore(name='topic_kernel_score', class_id='ngramm', \n",
    "                                                probability_mass_threshold=p_mass_threshold))\n",
    "    artm_model.scores.add(artm.TopTokensScore(name='top_tokens_score', class_id='ngramm', num_tokens=n_top_tokens))\n",
    "def fit_one_model(model, _n_iterations, _model_name=''): \n",
    "    print '[{}] fitting'.format(datetime.now())\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=_n_iterations)\n",
    "    print '[{}] outputting'.format(datetime.now())\n",
    "    printer.print_artm_model(model, _model_name, _n_iterations, output_file=models_file)\n",
    "    model_pics_file_name =  path.join(config.experiment_path, _model_name)\n",
    "    plot_maker.make_tm_plots(model, model_pics_file_name)\n",
    "    model_output_file_name = path.join(config.experiment_path, _model_name + '.txt')\n",
    "    printer.print_scores(model, _model_name, _n_iterations, model_output_file_name)\n",
    "    printer.print_top_tokens(model, model_output_file_name)\n",
    "    return model\n",
    "def save_pickle_file(dists, filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'wb')\n",
    "    pickle.dump(dists, pickle_file)\n",
    "    pickle_file.close()\n",
    "def load_pickle_file(filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'rb')\n",
    "    p_file = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    return p_file\n",
    "def save_model_pickle(_model_name, _model, _save=True):\n",
    "    phi = _model.get_phi()\n",
    "    phi = phi[(phi.T != 0).any()]\n",
    "    theta = _model.get_theta()    \n",
    "    saved_top_tokens = _model.score_tracker['top_tokens_score'].last_tokens\n",
    "    if _save:\n",
    "        save_pickle_file(phi, 'phi_{}.p'.format(_model_name))\n",
    "        save_pickle_file(theta, 'theta_{}.p'.format(_model_name))\n",
    "        save_pickle_file(saved_top_tokens, 'saved_top_tokens_{}.p'.format(_model_name))\n",
    "    return phi, theta, saved_top_tokens\n",
    "def load_model_pickle(_model_name, _distance_name):\n",
    "    phi = load_pickle_file('phi_{}.p'.format(_model_name))\n",
    "    theta = load_pickle_file('theta_{}.p'.format(_model_name))\n",
    "    saved_top_tokens = load_pickle_file('saved_top_tokens_{}.p'.format(_model_name))\n",
    "    distances = load_pickle_file('{}.p'.format(_distance_name))\n",
    "    return phi, theta, saved_top_tokens, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=config.output_batches_path,\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path=config.dictionary_path + '.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-17 17:27:30.048000] creating model\n",
      "[2017-01-17 17:27:31.568000] adding scores\n",
      "[2017-01-17 17:27:31.573000] fitting\n",
      "[2017-01-17 17:28:03.547000] outputting\n",
      "name = model_100_1, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_1')\n",
    "model1 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-17 17:42:44.448000] processing model 0 / 6\n",
      "[2017-01-17 17:42:44.449000] creating model\n",
      "[2017-01-17 17:42:45.994000] adding scores\n",
      "[2017-01-17 17:42:46.033000] fitting\n",
      "[2017-01-17 17:43:21.728000] outputting\n",
      "name = model_100_decor_0, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 17:43:38.897000] processing model 1 / 6\n",
      "[2017-01-17 17:43:38.897000] creating model\n",
      "[2017-01-17 17:43:40.265000] adding scores\n",
      "[2017-01-17 17:43:40.302000] fitting\n",
      "[2017-01-17 17:44:15.196000] outputting\n",
      "name = model_100_decor_1, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 17:44:32.554000] processing model 2 / 6\n",
      "[2017-01-17 17:44:32.555000] creating model\n",
      "[2017-01-17 17:44:33.944000] adding scores\n",
      "[2017-01-17 17:44:33.979000] fitting\n",
      "[2017-01-17 17:45:09.815000] outputting\n",
      "name = model_100_decor_2, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 17:45:27.453000] processing model 3 / 6\n",
      "[2017-01-17 17:45:27.454000] creating model\n",
      "[2017-01-17 17:45:28.989000] adding scores\n",
      "[2017-01-17 17:45:29.020000] fitting\n",
      "[2017-01-17 17:46:04.366000] outputting\n",
      "name = model_100_decor_3, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 17:46:21.970000] processing model 4 / 6\n",
      "[2017-01-17 17:46:21.971000] creating model\n",
      "[2017-01-17 17:46:23.375000] adding scores\n",
      "[2017-01-17 17:46:23.412000] fitting\n",
      "[2017-01-17 17:46:58.054000] outputting\n",
      "name = model_100_decor_4, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 17:47:16.329000] processing model 5 / 6\n",
      "[2017-01-17 17:47:16.330000] creating model\n",
      "[2017-01-17 17:47:17.962000] adding scores\n",
      "[2017-01-17 17:47:18.004000] fitting\n",
      "[2017-01-17 17:47:48.786000] outputting\n",
      "name = model_100_decor_5, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 10000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taus = [1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "for idx, t in enumerate(taus):\n",
    "    print '[{}] processing model {} / {}'.format(datetime.now(), idx, len(taus))\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                                n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "    tmp_model.regularizers['decorrelator_phi_regularizer'].tau = t\n",
    "    tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_decor_{}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-17 17:47:51.102000] processing model 0 / 13\n",
      "[2017-01-17 17:47:51.104000] creating model\n",
      "[2017-01-17 17:47:52.493000] adding scores\n",
      "[2017-01-17 17:47:52.525000] fitting\n",
      "[2017-01-17 17:48:21.335000] outputting\n",
      "name = model_100_ss_th_0, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 17:48:23.430000] processing model 1 / 13\n",
      "[2017-01-17 17:48:23.431000] creating model\n",
      "[2017-01-17 17:48:24.803000] adding scores\n",
      "[2017-01-17 17:48:24.833000] fitting\n",
      "[2017-01-17 17:48:53.993000] outputting\n",
      "name = model_100_ss_th_1, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 17:48:56.049000] processing model 2 / 13\n",
      "[2017-01-17 17:48:56.050000] creating model\n",
      "[2017-01-17 17:48:57.311000] adding scores\n",
      "[2017-01-17 17:48:57.340000] fitting\n",
      "[2017-01-17 17:49:30.084000] outputting\n",
      "name = model_100_ss_th_2, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 17:49:48.383000] processing model 3 / 13\n",
      "[2017-01-17 17:49:48.383000] creating model\n",
      "[2017-01-17 17:49:49.828000] adding scores\n",
      "[2017-01-17 17:49:49.859000] fitting\n",
      "[2017-01-17 17:50:23.438000] outputting\n",
      "name = model_100_ss_th_3, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 17:50:41.157000] processing model 4 / 13\n",
      "[2017-01-17 17:50:41.158000] creating model\n",
      "[2017-01-17 17:50:42.580000] adding scores\n",
      "[2017-01-17 17:50:42.616000] fitting\n",
      "[2017-01-17 17:51:16.551000] outputting\n",
      "name = model_100_ss_th_4, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 17:51:34.037000] processing model 5 / 13\n",
      "[2017-01-17 17:51:34.038000] creating model\n",
      "[2017-01-17 17:51:35.397000] adding scores\n",
      "[2017-01-17 17:51:35.427000] fitting\n",
      "[2017-01-17 17:52:09.209000] outputting\n",
      "name = model_100_ss_th_5, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 17:52:26.262000] processing model 6 / 13\n",
      "[2017-01-17 17:52:26.263000] creating model\n",
      "[2017-01-17 17:52:27.631000] adding scores\n",
      "[2017-01-17 17:52:27.666000] fitting\n",
      "[2017-01-17 17:53:01.424000] outputting\n",
      "name = model_100_ss_th_6, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 17:53:18.517000] processing model 7 / 13\n",
      "[2017-01-17 17:53:18.518000] creating model\n",
      "[2017-01-17 17:53:19.883000] adding scores\n",
      "[2017-01-17 17:53:19.914000] fitting\n",
      "[2017-01-17 17:53:53.993000] outputting\n",
      "name = model_100_ss_th_7, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 17:54:10.965000] processing model 8 / 13\n",
      "[2017-01-17 17:54:10.966000] creating model\n",
      "[2017-01-17 17:54:12.366000] adding scores\n",
      "[2017-01-17 17:54:12.395000] fitting\n",
      "[2017-01-17 17:54:45.588000] outputting\n",
      "name = model_100_ss_th_8, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 17:54:52.054000] processing model 9 / 13\n",
      "[2017-01-17 17:54:52.054000] creating model\n",
      "[2017-01-17 17:54:53.459000] adding scores\n",
      "[2017-01-17 17:54:53.495000] fitting\n",
      "[2017-01-17 17:55:26.787000] outputting\n",
      "name = model_100_ss_th_9, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 17:55:28.877000] processing model 10 / 13\n",
      "[2017-01-17 17:55:28.878000] creating model\n",
      "[2017-01-17 17:55:30.230000] adding scores\n",
      "[2017-01-17 17:55:30.259000] fitting\n",
      "[2017-01-17 17:56:03.812000] outputting\n",
      "name = model_100_ss_th_10, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 17:56:05.925000] processing model 11 / 13\n",
      "[2017-01-17 17:56:05.925000] creating model\n",
      "[2017-01-17 17:56:07.188000] adding scores\n",
      "[2017-01-17 17:56:07.217000] fitting\n",
      "[2017-01-17 17:56:40.530000] outputting\n",
      "name = model_100_ss_th_11, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 17:56:42.652000] processing model 12 / 13\n",
      "[2017-01-17 17:56:42.653000] creating model\n",
      "[2017-01-17 17:56:44.010000] adding scores\n",
      "[2017-01-17 17:56:44.041000] fitting\n",
      "[2017-01-17 17:57:17.656000] outputting\n",
      "name = model_100_ss_th_12, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taus = [-1e+5, -1e+3, -1e+1, -1, -1e-1, -1e-2, -1e-5, 1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "for idx, t in enumerate(taus):\n",
    "    print '[{}] processing model {} / {}'.format(datetime.now(), idx, len(taus))\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                                n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "    tmp_model.regularizers['ss_theta_regularizer'].tau = t\n",
    "    tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_ss_th_{}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-17 17:57:19.738000] processing model 0 / 13\n",
      "[2017-01-17 17:57:19.739000] creating model\n",
      "[2017-01-17 17:57:21.143000] adding scores\n",
      "[2017-01-17 17:57:21.172000] fitting\n",
      "[2017-01-17 17:57:50.952000] outputting\n",
      "name = model_100_ss_phi_0, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 17:57:53.036000] processing model 1 / 13\n",
      "[2017-01-17 17:57:53.036000] creating model\n",
      "[2017-01-17 17:57:54.366000] adding scores\n",
      "[2017-01-17 17:57:54.394000] fitting\n",
      "[2017-01-17 17:58:23.833000] outputting\n",
      "name = model_100_ss_phi_1, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 17:58:25.939000] processing model 2 / 13\n",
      "[2017-01-17 17:58:25.939000] creating model\n",
      "[2017-01-17 17:58:27.200000] adding scores\n",
      "[2017-01-17 17:58:27.230000] fitting\n",
      "[2017-01-17 17:58:58.619000] outputting\n",
      "name = model_100_ss_phi_2, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 17:59:01.251000] processing model 3 / 13\n",
      "[2017-01-17 17:59:01.252000] creating model\n",
      "[2017-01-17 17:59:02.542000] adding scores\n",
      "[2017-01-17 17:59:02.573000] fitting\n",
      "[2017-01-17 17:59:35.566000] outputting\n",
      "name = model_100_ss_phi_3, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 17:59:41.558000] processing model 4 / 13\n",
      "[2017-01-17 17:59:41.558000] creating model\n",
      "[2017-01-17 17:59:42.967000] adding scores\n",
      "[2017-01-17 17:59:42.997000] fitting\n",
      "[2017-01-17 18:00:16.239000] outputting\n",
      "name = model_100_ss_phi_4, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 18:00:40.470000] processing model 5 / 13\n",
      "[2017-01-17 18:00:40.470000] creating model\n",
      "[2017-01-17 18:00:41.812000] adding scores\n",
      "[2017-01-17 18:00:41.850000] fitting\n",
      "[2017-01-17 18:01:15.574000] outputting\n",
      "name = model_100_ss_phi_5, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 18:01:34.302000] processing model 6 / 13\n",
      "[2017-01-17 18:01:34.302000] creating model\n",
      "[2017-01-17 18:01:35.725000] adding scores\n",
      "[2017-01-17 18:01:35.756000] fitting\n",
      "[2017-01-17 18:02:09.810000] outputting\n",
      "name = model_100_ss_phi_6, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 18:02:26.791000] processing model 7 / 13\n",
      "[2017-01-17 18:02:26.792000] creating model\n",
      "[2017-01-17 18:02:28.148000] adding scores\n",
      "[2017-01-17 18:02:28.179000] fitting\n",
      "[2017-01-17 18:03:02.816000] outputting\n",
      "name = model_100_ss_phi_7, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 18:03:11.158000] processing model 8 / 13\n",
      "[2017-01-17 18:03:11.158000] creating model\n",
      "[2017-01-17 18:03:12.490000] adding scores\n",
      "[2017-01-17 18:03:12.521000] fitting\n",
      "[2017-01-17 18:03:46.951000] outputting\n",
      "name = model_100_ss_phi_8, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 18:04:05.156000] processing model 9 / 13\n",
      "[2017-01-17 18:04:05.156000] creating model\n",
      "[2017-01-17 18:04:06.575000] adding scores\n",
      "[2017-01-17 18:04:06.604000] fitting\n",
      "[2017-01-17 18:04:41.088000] outputting\n",
      "name = model_100_ss_phi_9, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 18:04:43.180000] processing model 10 / 13\n",
      "[2017-01-17 18:04:43.181000] creating model\n",
      "[2017-01-17 18:04:44.529000] adding scores\n",
      "[2017-01-17 18:04:44.561000] fitting\n",
      "[2017-01-17 18:05:19.127000] outputting\n",
      "name = model_100_ss_phi_10, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 18:05:21.235000] processing model 11 / 13\n",
      "[2017-01-17 18:05:21.236000] creating model\n",
      "[2017-01-17 18:05:22.507000] adding scores\n",
      "[2017-01-17 18:05:22.538000] fitting\n",
      "[2017-01-17 18:05:58.039000] outputting\n",
      "name = model_100_ss_phi_11, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 18:06:00.074000] processing model 12 / 13\n",
      "[2017-01-17 18:06:00.075000] creating model\n",
      "[2017-01-17 18:06:01.455000] adding scores\n",
      "[2017-01-17 18:06:01.486000] fitting\n",
      "[2017-01-17 18:06:34.516000] outputting\n",
      "name = model_100_ss_phi_12, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taus = [-1e+5, -1e+3, -1e+1, -1, -1e-1, -1e-2, -1e-5, 1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "for idx, t in enumerate(taus):\n",
    "    print '[{}] processing model {} / {}'.format(datetime.now(), idx, len(taus))\n",
    "    tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                                n_top_tokens=15, p_mass_threshold=0.25)\n",
    "    tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "    tmp_model.regularizers['ss_phi_regularizer'].tau = t\n",
    "    tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_ss_phi_{}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-17 19:20:25.995000] processing model 0 / 26\n",
      "[2017-01-17 19:20:25.995000] creating model\n",
      "[2017-01-17 19:20:27.497000] adding scores\n",
      "[2017-01-17 19:20:27.505000] fitting\n",
      "[2017-01-17 19:20:58.780000] outputting\n",
      "name = model_100_ss_th_ss_phi_0, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 19:21:00.869000] processing model 1 / 26\n",
      "[2017-01-17 19:21:00.869000] creating model\n",
      "[2017-01-17 19:21:02.172000] adding scores\n",
      "[2017-01-17 19:21:02.204000] fitting\n",
      "[2017-01-17 19:21:32.829000] outputting\n",
      "name = model_100_ss_th_ss_phi_1, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 19:21:34.971000] processing model 2 / 26\n",
      "[2017-01-17 19:21:34.972000] creating model\n",
      "[2017-01-17 19:21:36.345000] adding scores\n",
      "[2017-01-17 19:21:36.376000] fitting\n",
      "[2017-01-17 19:22:07.131000] outputting\n",
      "name = model_100_ss_th_ss_phi_2, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 19:22:09.175000] processing model 3 / 26\n",
      "[2017-01-17 19:22:09.175000] creating model\n",
      "[2017-01-17 19:22:10.584000] adding scores\n",
      "[2017-01-17 19:22:10.614000] fitting\n",
      "[2017-01-17 19:22:40.861000] outputting\n",
      "name = model_100_ss_th_ss_phi_3, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 19:22:42.932000] processing model 4 / 26\n",
      "[2017-01-17 19:22:42.933000] creating model\n",
      "[2017-01-17 19:22:44.360000] adding scores\n",
      "[2017-01-17 19:22:44.392000] fitting\n",
      "[2017-01-17 19:23:14.946000] outputting\n",
      "name = model_100_ss_th_ss_phi_4, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 19:23:17.056000] processing model 5 / 26\n",
      "[2017-01-17 19:23:17.057000] creating model\n",
      "[2017-01-17 19:23:18.409000] adding scores\n",
      "[2017-01-17 19:23:18.441000] fitting\n",
      "[2017-01-17 19:23:50.962000] outputting\n",
      "name = model_100_ss_th_ss_phi_5, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 19:23:53.065000] processing model 6 / 26\n",
      "[2017-01-17 19:23:53.065000] creating model\n",
      "[2017-01-17 19:23:54.437000] adding scores\n",
      "[2017-01-17 19:23:54.471000] fitting\n",
      "[2017-01-17 19:24:24.870000] outputting\n",
      "name = model_100_ss_th_ss_phi_6, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 19:24:26.966000] processing model 7 / 26\n",
      "[2017-01-17 19:24:26.966000] creating model\n",
      "[2017-01-17 19:24:28.372000] adding scores\n",
      "[2017-01-17 19:24:28.403000] fitting\n",
      "[2017-01-17 19:25:08.171000] outputting\n",
      "name = model_100_ss_th_ss_phi_7, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 19:25:10.263000] processing model 8 / 26\n",
      "[2017-01-17 19:25:10.264000] creating model\n",
      "[2017-01-17 19:25:11.621000] adding scores\n",
      "[2017-01-17 19:25:11.652000] fitting\n",
      "[2017-01-17 19:25:47.574000] outputting\n",
      "name = model_100_ss_th_ss_phi_8, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 19:25:49.539000] processing model 9 / 26\n",
      "[2017-01-17 19:25:49.540000] creating model\n",
      "[2017-01-17 19:25:51.011000] adding scores\n",
      "[2017-01-17 19:25:51.042000] fitting\n",
      "[2017-01-17 19:26:23.769000] outputting\n",
      "name = model_100_ss_th_ss_phi_9, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 19:26:25.974000] processing model 10 / 26\n",
      "[2017-01-17 19:26:25.975000] creating model\n",
      "[2017-01-17 19:26:27.230000] adding scores\n",
      "[2017-01-17 19:26:27.261000] fitting\n",
      "[2017-01-17 19:26:59.240000] outputting\n",
      "name = model_100_ss_th_ss_phi_10, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 19:27:02.571000] processing model 11 / 26\n",
      "[2017-01-17 19:27:02.572000] creating model\n",
      "[2017-01-17 19:27:04.294000] adding scores\n",
      "[2017-01-17 19:27:04.325000] fitting\n",
      "[2017-01-17 19:27:39.450000] outputting\n",
      "name = model_100_ss_th_ss_phi_11, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 19:27:41.506000] processing model 12 / 26\n",
      "[2017-01-17 19:27:41.507000] creating model\n",
      "[2017-01-17 19:27:42.849000] adding scores\n",
      "[2017-01-17 19:27:42.881000] fitting\n",
      "[2017-01-17 19:28:16.437000] outputting\n",
      "name = model_100_ss_th_ss_phi_12, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -100000.0\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 19:28:18.483000] processing model 13 / 26\n",
      "[2017-01-17 19:28:18.484000] creating model\n",
      "[2017-01-17 19:28:19.837000] adding scores\n",
      "[2017-01-17 19:28:19.874000] fitting\n",
      "[2017-01-17 19:28:52.484000] outputting\n",
      "name = model_100_ss_th_ss_phi_13, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 19:28:54.530000] processing model 14 / 26\n",
      "[2017-01-17 19:28:54.530000] creating model\n",
      "[2017-01-17 19:28:55.957000] adding scores\n",
      "[2017-01-17 19:28:55.991000] fitting\n",
      "[2017-01-17 19:29:28.139000] outputting\n",
      "name = model_100_ss_th_ss_phi_14, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 19:29:30.157000] processing model 15 / 26\n",
      "[2017-01-17 19:29:30.157000] creating model\n",
      "[2017-01-17 19:29:31.464000] adding scores\n",
      "[2017-01-17 19:29:31.495000] fitting\n",
      "[2017-01-17 19:30:03.646000] outputting\n",
      "name = model_100_ss_th_ss_phi_15, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 19:30:05.760000] processing model 16 / 26\n",
      "[2017-01-17 19:30:05.761000] creating model\n",
      "[2017-01-17 19:30:07.195000] adding scores\n",
      "[2017-01-17 19:30:07.229000] fitting\n",
      "[2017-01-17 19:30:40.520000] outputting\n",
      "name = model_100_ss_th_ss_phi_16, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 19:30:42.707000] processing model 17 / 26\n",
      "[2017-01-17 19:30:42.707000] creating model\n",
      "[2017-01-17 19:30:44.067000] adding scores\n",
      "[2017-01-17 19:30:44.099000] fitting\n",
      "[2017-01-17 19:31:16.386000] outputting\n",
      "name = model_100_ss_th_ss_phi_17, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 19:31:18.349000] processing model 18 / 26\n",
      "[2017-01-17 19:31:18.350000] creating model\n",
      "[2017-01-17 19:31:19.721000] adding scores\n",
      "[2017-01-17 19:31:19.790000] fitting\n",
      "[2017-01-17 19:31:52.367000] outputting\n",
      "name = model_100_ss_th_ss_phi_18, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 19:31:54.400000] processing model 19 / 26\n",
      "[2017-01-17 19:31:54.400000] creating model\n",
      "[2017-01-17 19:31:55.715000] adding scores\n",
      "[2017-01-17 19:31:55.748000] fitting\n",
      "[2017-01-17 19:32:27.148000] outputting\n",
      "name = model_100_ss_th_ss_phi_19, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 19:32:29.229000] processing model 20 / 26\n",
      "[2017-01-17 19:32:29.230000] creating model\n",
      "[2017-01-17 19:32:30.545000] adding scores\n",
      "[2017-01-17 19:32:30.574000] fitting\n",
      "[2017-01-17 19:33:05.643000] outputting\n",
      "name = model_100_ss_th_ss_phi_20, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 19:33:07.746000] processing model 21 / 26\n",
      "[2017-01-17 19:33:07.746000] creating model\n",
      "[2017-01-17 19:33:09.069000] adding scores\n",
      "[2017-01-17 19:33:09.100000] fitting\n",
      "[2017-01-17 19:33:41.527000] outputting\n",
      "name = model_100_ss_th_ss_phi_21, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 19:33:43.529000] processing model 22 / 26\n",
      "[2017-01-17 19:33:43.530000] creating model\n",
      "[2017-01-17 19:33:44.872000] adding scores\n",
      "[2017-01-17 19:33:44.904000] fitting\n",
      "[2017-01-17 19:34:17.030000] outputting\n",
      "name = model_100_ss_th_ss_phi_22, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 19:34:19.115000] processing model 23 / 26\n",
      "[2017-01-17 19:34:19.115000] creating model\n",
      "[2017-01-17 19:34:20.539000] adding scores\n",
      "[2017-01-17 19:34:20.569000] fitting\n",
      "[2017-01-17 19:34:51.193000] outputting\n",
      "name = model_100_ss_th_ss_phi_23, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 19:34:53.255000] processing model 24 / 26\n",
      "[2017-01-17 19:34:53.255000] creating model\n",
      "[2017-01-17 19:34:54.555000] adding scores\n",
      "[2017-01-17 19:34:54.588000] fitting\n",
      "[2017-01-17 19:35:26.565000] outputting\n",
      "name = model_100_ss_th_ss_phi_24, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 19:35:28.555000] processing model 25 / 26\n",
      "[2017-01-17 19:35:28.555000] creating model\n",
      "[2017-01-17 19:35:29.850000] adding scores\n",
      "[2017-01-17 19:35:29.881000] fitting\n",
      "[2017-01-17 19:36:02.334000] outputting\n",
      "name = model_100_ss_th_ss_phi_25, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1000.0\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 19:36:04.419000] processing model 26 / 26\n",
      "[2017-01-17 19:36:04.420000] creating model\n",
      "[2017-01-17 19:36:05.802000] adding scores\n",
      "[2017-01-17 19:36:05.835000] fitting\n",
      "[2017-01-17 19:36:35.798000] outputting\n",
      "name = model_100_ss_th_ss_phi_26, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 19:36:37.914000] processing model 27 / 26\n",
      "[2017-01-17 19:36:37.914000] creating model\n",
      "[2017-01-17 19:36:39.266000] adding scores\n",
      "[2017-01-17 19:36:39.303000] fitting\n",
      "[2017-01-17 19:37:12.600000] outputting\n",
      "name = model_100_ss_th_ss_phi_27, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 19:37:14.679000] processing model 28 / 26\n",
      "[2017-01-17 19:37:14.680000] creating model\n",
      "[2017-01-17 19:37:16.043000] adding scores\n",
      "[2017-01-17 19:37:16.076000] fitting\n",
      "[2017-01-17 19:37:48.313000] outputting\n",
      "name = model_100_ss_th_ss_phi_28, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 19:37:50.813000] processing model 29 / 26\n",
      "[2017-01-17 19:37:50.814000] creating model\n",
      "[2017-01-17 19:37:52.146000] adding scores\n",
      "[2017-01-17 19:37:52.179000] fitting\n",
      "[2017-01-17 19:38:25.978000] outputting\n",
      "name = model_100_ss_th_ss_phi_29, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 19:38:32.814000] processing model 30 / 26\n",
      "[2017-01-17 19:38:32.814000] creating model\n",
      "[2017-01-17 19:38:34.250000] adding scores\n",
      "[2017-01-17 19:38:34.283000] fitting\n",
      "[2017-01-17 19:39:08.449000] outputting\n",
      "name = model_100_ss_th_ss_phi_30, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 19:39:27.625000] processing model 31 / 26\n",
      "[2017-01-17 19:39:27.626000] creating model\n",
      "[2017-01-17 19:39:29.007000] adding scores\n",
      "[2017-01-17 19:39:29.039000] fitting\n",
      "[2017-01-17 19:40:09.975000] outputting\n",
      "name = model_100_ss_th_ss_phi_31, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 19:40:30.106000] processing model 32 / 26\n",
      "[2017-01-17 19:40:30.106000] creating model\n",
      "[2017-01-17 19:40:31.449000] adding scores\n",
      "[2017-01-17 19:40:31.481000] fitting\n",
      "[2017-01-17 19:41:08.767000] outputting\n",
      "name = model_100_ss_th_ss_phi_32, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 19:41:28.304000] processing model 33 / 26\n",
      "[2017-01-17 19:41:28.305000] creating model\n",
      "[2017-01-17 19:41:29.569000] adding scores\n",
      "[2017-01-17 19:41:29.601000] fitting\n",
      "[2017-01-17 19:42:05.601000] outputting\n",
      "name = model_100_ss_th_ss_phi_33, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 19:42:16.028000] processing model 34 / 26\n",
      "[2017-01-17 19:42:16.028000] creating model\n",
      "[2017-01-17 19:42:17.412000] adding scores\n",
      "[2017-01-17 19:42:17.445000] fitting\n",
      "[2017-01-17 19:42:53.137000] outputting\n",
      "name = model_100_ss_th_ss_phi_34, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 19:43:14.525000] processing model 35 / 26\n",
      "[2017-01-17 19:43:14.525000] creating model\n",
      "[2017-01-17 19:43:15.962000] adding scores\n",
      "[2017-01-17 19:43:15.994000] fitting\n",
      "[2017-01-17 19:43:50.087000] outputting\n",
      "name = model_100_ss_th_ss_phi_35, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 19:44:10.393000] processing model 36 / 26\n",
      "[2017-01-17 19:44:10.394000] creating model\n",
      "[2017-01-17 19:44:11.711000] adding scores\n",
      "[2017-01-17 19:44:11.745000] fitting\n",
      "[2017-01-17 19:44:46.006000] outputting\n",
      "name = model_100_ss_th_ss_phi_36, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 19:45:05.859000] processing model 37 / 26\n",
      "[2017-01-17 19:45:05.859000] creating model\n",
      "[2017-01-17 19:45:07.170000] adding scores\n",
      "[2017-01-17 19:45:07.202000] fitting\n",
      "[2017-01-17 19:45:40.511000] outputting\n",
      "name = model_100_ss_th_ss_phi_37, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 19:45:42.576000] processing model 38 / 26\n",
      "[2017-01-17 19:45:42.576000] creating model\n",
      "[2017-01-17 19:45:43.982000] adding scores\n",
      "[2017-01-17 19:45:44.012000] fitting\n",
      "[2017-01-17 19:46:16.063000] outputting\n",
      "name = model_100_ss_th_ss_phi_38, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -10.0\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 19:46:18.012000] processing model 39 / 26\n",
      "[2017-01-17 19:46:18.013000] creating model\n",
      "[2017-01-17 19:46:19.345000] adding scores\n",
      "[2017-01-17 19:46:19.377000] fitting\n",
      "[2017-01-17 19:46:51.496000] outputting\n",
      "name = model_100_ss_th_ss_phi_39, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 19:46:55.226000] processing model 40 / 26\n",
      "[2017-01-17 19:46:55.226000] creating model\n",
      "[2017-01-17 19:46:56.593000] adding scores\n",
      "[2017-01-17 19:46:56.624000] fitting\n",
      "[2017-01-17 19:47:26.681000] outputting\n",
      "name = model_100_ss_th_ss_phi_40, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 19:47:28.741000] processing model 41 / 26\n",
      "[2017-01-17 19:47:28.741000] creating model\n",
      "[2017-01-17 19:47:30.079000] adding scores\n",
      "[2017-01-17 19:47:30.112000] fitting\n",
      "[2017-01-17 19:48:03.849000] outputting\n",
      "name = model_100_ss_th_ss_phi_41, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 19:48:08.410000] processing model 42 / 26\n",
      "[2017-01-17 19:48:08.410000] creating model\n",
      "[2017-01-17 19:48:09.729000] adding scores\n",
      "[2017-01-17 19:48:09.760000] fitting\n",
      "[2017-01-17 19:48:42.624000] outputting\n",
      "name = model_100_ss_th_ss_phi_42, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 19:48:51.628000] processing model 43 / 26\n",
      "[2017-01-17 19:48:51.628000] creating model\n",
      "[2017-01-17 19:48:52.955000] adding scores\n",
      "[2017-01-17 19:48:52.986000] fitting\n",
      "[2017-01-17 19:49:29.403000] outputting\n",
      "name = model_100_ss_th_ss_phi_43, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 19:49:54.343000] processing model 44 / 26\n",
      "[2017-01-17 19:49:54.344000] creating model\n",
      "[2017-01-17 19:49:55.681000] adding scores\n",
      "[2017-01-17 19:49:55.712000] fitting\n",
      "[2017-01-17 19:50:30.663000] outputting\n",
      "name = model_100_ss_th_ss_phi_44, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 19:50:49.926000] processing model 45 / 26\n",
      "[2017-01-17 19:50:49.927000] creating model\n",
      "[2017-01-17 19:50:51.198000] adding scores\n",
      "[2017-01-17 19:50:51.230000] fitting\n",
      "[2017-01-17 19:51:27.480000] outputting\n",
      "name = model_100_ss_th_ss_phi_45, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 19:51:45.030000] processing model 46 / 26\n",
      "[2017-01-17 19:51:45.031000] creating model\n",
      "[2017-01-17 19:51:46.420000] adding scores\n",
      "[2017-01-17 19:51:46.451000] fitting\n",
      "[2017-01-17 19:52:20.698000] outputting\n",
      "name = model_100_ss_th_ss_phi_46, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 19:52:29.079000] processing model 47 / 26\n",
      "[2017-01-17 19:52:29.080000] creating model\n",
      "[2017-01-17 19:52:30.448000] adding scores\n",
      "[2017-01-17 19:52:30.478000] fitting\n",
      "[2017-01-17 19:53:06.701000] outputting\n",
      "name = model_100_ss_th_ss_phi_47, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 19:53:29.823000] processing model 48 / 26\n",
      "[2017-01-17 19:53:29.823000] creating model\n",
      "[2017-01-17 19:53:31.178000] adding scores\n",
      "[2017-01-17 19:53:31.209000] fitting\n",
      "[2017-01-17 19:54:08.075000] outputting\n",
      "name = model_100_ss_th_ss_phi_48, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 19:54:26.232000] processing model 49 / 26\n",
      "[2017-01-17 19:54:26.233000] creating model\n",
      "[2017-01-17 19:54:27.636000] adding scores\n",
      "[2017-01-17 19:54:27.668000] fitting\n",
      "[2017-01-17 19:55:02.458000] outputting\n",
      "name = model_100_ss_th_ss_phi_49, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 19:55:04.521000] processing model 50 / 26\n",
      "[2017-01-17 19:55:04.522000] creating model\n",
      "[2017-01-17 19:55:05.772000] adding scores\n",
      "[2017-01-17 19:55:05.804000] fitting\n",
      "[2017-01-17 19:55:42.207000] outputting\n",
      "name = model_100_ss_th_ss_phi_50, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 19:55:44.281000] processing model 51 / 26\n",
      "[2017-01-17 19:55:44.282000] creating model\n",
      "[2017-01-17 19:55:45.646000] adding scores\n",
      "[2017-01-17 19:55:45.678000] fitting\n",
      "[2017-01-17 19:56:19.605000] outputting\n",
      "name = model_100_ss_th_ss_phi_51, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 19:56:21.689000] processing model 52 / 26\n",
      "[2017-01-17 19:56:21.689000] creating model\n",
      "[2017-01-17 19:56:23.021000] adding scores\n",
      "[2017-01-17 19:56:23.052000] fitting\n",
      "[2017-01-17 19:56:54.837000] outputting\n",
      "name = model_100_ss_th_ss_phi_52, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 19:56:56.889000] processing model 53 / 26\n",
      "[2017-01-17 19:56:56.890000] creating model\n",
      "[2017-01-17 19:56:58.286000] adding scores\n",
      "[2017-01-17 19:56:58.321000] fitting\n",
      "[2017-01-17 19:57:29.816000] outputting\n",
      "name = model_100_ss_th_ss_phi_53, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 19:57:31.869000] processing model 54 / 26\n",
      "[2017-01-17 19:57:31.869000] creating model\n",
      "[2017-01-17 19:57:33.289000] adding scores\n",
      "[2017-01-17 19:57:33.320000] fitting\n",
      "[2017-01-17 19:58:06.755000] outputting\n",
      "name = model_100_ss_th_ss_phi_54, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 19:58:09.391000] processing model 55 / 26\n",
      "[2017-01-17 19:58:09.392000] creating model\n",
      "[2017-01-17 19:58:10.756000] adding scores\n",
      "[2017-01-17 19:58:10.793000] fitting\n",
      "[2017-01-17 19:58:43.586000] outputting\n",
      "name = model_100_ss_th_ss_phi_55, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 19:58:50.135000] processing model 56 / 26\n",
      "[2017-01-17 19:58:50.135000] creating model\n",
      "[2017-01-17 19:58:51.429000] adding scores\n",
      "[2017-01-17 19:58:51.458000] fitting\n",
      "[2017-01-17 19:59:28.996000] outputting\n",
      "name = model_100_ss_th_ss_phi_56, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 19:59:53.414000] processing model 57 / 26\n",
      "[2017-01-17 19:59:53.415000] creating model\n",
      "[2017-01-17 19:59:54.783000] adding scores\n",
      "[2017-01-17 19:59:54.822000] fitting\n",
      "[2017-01-17 20:00:29.096000] outputting\n",
      "name = model_100_ss_th_ss_phi_57, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 20:00:48.335000] processing model 58 / 26\n",
      "[2017-01-17 20:00:48.335000] creating model\n",
      "[2017-01-17 20:00:49.684000] adding scores\n",
      "[2017-01-17 20:00:49.723000] fitting\n",
      "[2017-01-17 20:01:25.159000] outputting\n",
      "name = model_100_ss_th_ss_phi_58, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 20:01:44.463000] processing model 59 / 26\n",
      "[2017-01-17 20:01:44.463000] creating model\n",
      "[2017-01-17 20:01:45.844000] adding scores\n",
      "[2017-01-17 20:01:45.876000] fitting\n",
      "[2017-01-17 20:02:22.147000] outputting\n",
      "name = model_100_ss_th_ss_phi_59, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 20:02:30.438000] processing model 60 / 26\n",
      "[2017-01-17 20:02:30.438000] creating model\n",
      "[2017-01-17 20:02:31.786000] adding scores\n",
      "[2017-01-17 20:02:31.817000] fitting\n",
      "[2017-01-17 20:03:08.797000] outputting\n",
      "name = model_100_ss_th_ss_phi_60, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 20:03:27.070000] processing model 61 / 26\n",
      "[2017-01-17 20:03:27.070000] creating model\n",
      "[2017-01-17 20:03:28.425000] adding scores\n",
      "[2017-01-17 20:03:28.456000] fitting\n",
      "[2017-01-17 20:04:05.405000] outputting\n",
      "name = model_100_ss_th_ss_phi_61, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 20:04:07.426000] processing model 62 / 26\n",
      "[2017-01-17 20:04:07.426000] creating model\n",
      "[2017-01-17 20:04:08.785000] adding scores\n",
      "[2017-01-17 20:04:08.819000] fitting\n",
      "[2017-01-17 20:04:44.460000] outputting\n",
      "name = model_100_ss_th_ss_phi_62, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 20:04:46.627000] processing model 63 / 26\n",
      "[2017-01-17 20:04:46.627000] creating model\n",
      "[2017-01-17 20:04:47.967000] adding scores\n",
      "[2017-01-17 20:04:47.998000] fitting\n",
      "[2017-01-17 20:05:25.939000] outputting\n",
      "name = model_100_ss_th_ss_phi_63, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 20:05:27.992000] processing model 64 / 26\n",
      "[2017-01-17 20:05:27.993000] creating model\n",
      "[2017-01-17 20:05:29.445000] adding scores\n",
      "[2017-01-17 20:05:29.483000] fitting\n",
      "[2017-01-17 20:06:04.590000] outputting\n",
      "name = model_100_ss_th_ss_phi_64, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.1\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 20:06:06.607000] processing model 65 / 26\n",
      "[2017-01-17 20:06:06.608000] creating model\n",
      "[2017-01-17 20:06:07.939000] adding scores\n",
      "[2017-01-17 20:06:07.970000] fitting\n",
      "[2017-01-17 20:06:39.523000] outputting\n",
      "name = model_100_ss_th_ss_phi_65, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 20:06:41.560000] processing model 66 / 26\n",
      "[2017-01-17 20:06:41.561000] creating model\n",
      "[2017-01-17 20:06:42.994000] adding scores\n",
      "[2017-01-17 20:06:43.032000] fitting\n",
      "[2017-01-17 20:07:13.382000] outputting\n",
      "name = model_100_ss_th_ss_phi_66, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 20:07:15.471000] processing model 67 / 26\n",
      "[2017-01-17 20:07:15.472000] creating model\n",
      "[2017-01-17 20:07:16.893000] adding scores\n",
      "[2017-01-17 20:07:16.930000] fitting\n",
      "[2017-01-17 20:07:52.081000] outputting\n",
      "name = model_100_ss_th_ss_phi_67, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 20:07:54.767000] processing model 68 / 26\n",
      "[2017-01-17 20:07:54.768000] creating model\n",
      "[2017-01-17 20:07:56.104000] adding scores\n",
      "[2017-01-17 20:07:56.135000] fitting\n",
      "[2017-01-17 20:08:30.542000] outputting\n",
      "name = model_100_ss_th_ss_phi_68, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 20:08:36.712000] processing model 69 / 26\n",
      "[2017-01-17 20:08:36.712000] creating model\n",
      "[2017-01-17 20:08:38.057000] adding scores\n",
      "[2017-01-17 20:08:38.087000] fitting\n",
      "[2017-01-17 20:09:12.282000] outputting\n",
      "name = model_100_ss_th_ss_phi_69, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 20:09:36.452000] processing model 70 / 26\n",
      "[2017-01-17 20:09:36.452000] creating model\n",
      "[2017-01-17 20:09:37.800000] adding scores\n",
      "[2017-01-17 20:09:37.830000] fitting\n",
      "[2017-01-17 20:10:15.881000] outputting\n",
      "name = model_100_ss_th_ss_phi_70, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 20:10:34.963000] processing model 71 / 26\n",
      "[2017-01-17 20:10:34.963000] creating model\n",
      "[2017-01-17 20:10:36.277000] adding scores\n",
      "[2017-01-17 20:10:36.309000] fitting\n",
      "[2017-01-17 20:11:12.766000] outputting\n",
      "name = model_100_ss_th_ss_phi_71, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 20:11:30.048000] processing model 72 / 26\n",
      "[2017-01-17 20:11:30.048000] creating model\n",
      "[2017-01-17 20:11:31.483000] adding scores\n",
      "[2017-01-17 20:11:31.516000] fitting\n",
      "[2017-01-17 20:12:07.836000] outputting\n",
      "name = model_100_ss_th_ss_phi_72, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 20:12:16.030000] processing model 73 / 26\n",
      "[2017-01-17 20:12:16.031000] creating model\n",
      "[2017-01-17 20:12:17.442000] adding scores\n",
      "[2017-01-17 20:12:17.475000] fitting\n",
      "[2017-01-17 20:12:53.211000] outputting\n",
      "name = model_100_ss_th_ss_phi_73, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 20:13:11.650000] processing model 74 / 26\n",
      "[2017-01-17 20:13:11.650000] creating model\n",
      "[2017-01-17 20:13:13.094000] adding scores\n",
      "[2017-01-17 20:13:13.131000] fitting\n",
      "[2017-01-17 20:13:49.067000] outputting\n",
      "name = model_100_ss_th_ss_phi_74, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 20:13:51.921000] processing model 75 / 26\n",
      "[2017-01-17 20:13:51.921000] creating model\n",
      "[2017-01-17 20:13:53.263000] adding scores\n",
      "[2017-01-17 20:13:53.295000] fitting\n",
      "[2017-01-17 20:14:29.721000] outputting\n",
      "name = model_100_ss_th_ss_phi_75, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 20:14:31.855000] processing model 76 / 26\n",
      "[2017-01-17 20:14:31.856000] creating model\n",
      "[2017-01-17 20:14:33.215000] adding scores\n",
      "[2017-01-17 20:14:33.253000] fitting\n",
      "[2017-01-17 20:15:08.341000] outputting\n",
      "name = model_100_ss_th_ss_phi_76, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 20:15:10.451000] processing model 77 / 26\n",
      "[2017-01-17 20:15:10.452000] creating model\n",
      "[2017-01-17 20:15:11.860000] adding scores\n",
      "[2017-01-17 20:15:11.892000] fitting\n",
      "[2017-01-17 20:15:48.579000] outputting\n",
      "name = model_100_ss_th_ss_phi_77, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.01\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 20:15:50.615000] processing model 78 / 26\n",
      "[2017-01-17 20:15:50.616000] creating model\n",
      "[2017-01-17 20:15:51.980000] adding scores\n",
      "[2017-01-17 20:15:52.015000] fitting\n",
      "[2017-01-17 20:16:22.137000] outputting\n",
      "name = model_100_ss_th_ss_phi_78, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 20:16:24.187000] processing model 79 / 26\n",
      "[2017-01-17 20:16:24.187000] creating model\n",
      "[2017-01-17 20:16:25.620000] adding scores\n",
      "[2017-01-17 20:16:25.653000] fitting\n",
      "[2017-01-17 20:16:57.857000] outputting\n",
      "name = model_100_ss_th_ss_phi_79, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 20:16:59.923000] processing model 80 / 26\n",
      "[2017-01-17 20:16:59.924000] creating model\n",
      "[2017-01-17 20:17:01.324000] adding scores\n",
      "[2017-01-17 20:17:01.355000] fitting\n",
      "[2017-01-17 20:17:35.035000] outputting\n",
      "name = model_100_ss_th_ss_phi_80, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 20:17:37.664000] processing model 81 / 26\n",
      "[2017-01-17 20:17:37.664000] creating model\n",
      "[2017-01-17 20:17:39.024000] adding scores\n",
      "[2017-01-17 20:17:39.058000] fitting\n",
      "[2017-01-17 20:18:15.945000] outputting\n",
      "name = model_100_ss_th_ss_phi_81, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 20:18:22.229000] processing model 82 / 26\n",
      "[2017-01-17 20:18:22.229000] creating model\n",
      "[2017-01-17 20:18:23.628000] adding scores\n",
      "[2017-01-17 20:18:23.659000] fitting\n",
      "[2017-01-17 20:19:03.654000] outputting\n",
      "name = model_100_ss_th_ss_phi_82, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 20:19:28.285000] processing model 83 / 26\n",
      "[2017-01-17 20:19:28.286000] creating model\n",
      "[2017-01-17 20:19:29.716000] adding scores\n",
      "[2017-01-17 20:19:29.751000] fitting\n",
      "[2017-01-17 20:20:05.778000] outputting\n",
      "name = model_100_ss_th_ss_phi_83, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 20:20:24.765000] processing model 84 / 26\n",
      "[2017-01-17 20:20:24.766000] creating model\n",
      "[2017-01-17 20:20:26.103000] adding scores\n",
      "[2017-01-17 20:20:26.135000] fitting\n",
      "[2017-01-17 20:21:02.606000] outputting\n",
      "name = model_100_ss_th_ss_phi_84, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 20:21:19.852000] processing model 85 / 26\n",
      "[2017-01-17 20:21:19.852000] creating model\n",
      "[2017-01-17 20:21:21.294000] adding scores\n",
      "[2017-01-17 20:21:21.327000] fitting\n",
      "[2017-01-17 20:21:57.093000] outputting\n",
      "name = model_100_ss_th_ss_phi_85, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 20:22:05.334000] processing model 86 / 26\n",
      "[2017-01-17 20:22:05.334000] creating model\n",
      "[2017-01-17 20:22:06.791000] adding scores\n",
      "[2017-01-17 20:22:06.822000] fitting\n",
      "[2017-01-17 20:22:43.361000] outputting\n",
      "name = model_100_ss_th_ss_phi_86, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 20:23:01.621000] processing model 87 / 26\n",
      "[2017-01-17 20:23:01.622000] creating model\n",
      "[2017-01-17 20:23:02.986000] adding scores\n",
      "[2017-01-17 20:23:03.020000] fitting\n",
      "[2017-01-17 20:23:38.757000] outputting\n",
      "name = model_100_ss_th_ss_phi_87, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 20:23:41.573000] processing model 88 / 26\n",
      "[2017-01-17 20:23:41.574000] creating model\n",
      "[2017-01-17 20:23:42.927000] adding scores\n",
      "[2017-01-17 20:23:42.959000] fitting\n",
      "[2017-01-17 20:24:19.653000] outputting\n",
      "name = model_100_ss_th_ss_phi_88, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 20:24:21.805000] processing model 89 / 26\n",
      "[2017-01-17 20:24:21.806000] creating model\n",
      "[2017-01-17 20:24:23.165000] adding scores\n",
      "[2017-01-17 20:24:23.198000] fitting\n",
      "[2017-01-17 20:24:59.920000] outputting\n",
      "name = model_100_ss_th_ss_phi_89, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 20:25:03.672000] processing model 90 / 26\n",
      "[2017-01-17 20:25:03.672000] creating model\n",
      "[2017-01-17 20:25:05.136000] adding scores\n",
      "[2017-01-17 20:25:05.173000] fitting\n",
      "[2017-01-17 20:25:38.754000] outputting\n",
      "name = model_100_ss_th_ss_phi_90, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1e-05\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 20:25:40.896000] processing model 91 / 26\n",
      "[2017-01-17 20:25:40.897000] creating model\n",
      "[2017-01-17 20:25:42.240000] adding scores\n",
      "[2017-01-17 20:25:42.275000] fitting\n",
      "[2017-01-17 20:26:12.580000] outputting\n",
      "name = model_100_ss_th_ss_phi_91, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 20:26:14.563000] processing model 92 / 26\n",
      "[2017-01-17 20:26:14.563000] creating model\n",
      "[2017-01-17 20:26:15.895000] adding scores\n",
      "[2017-01-17 20:26:15.927000] fitting\n",
      "[2017-01-17 20:26:47] outputting\n",
      "name = model_100_ss_th_ss_phi_92, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 20:26:49.009000] processing model 93 / 26\n",
      "[2017-01-17 20:26:49.010000] creating model\n",
      "[2017-01-17 20:26:50.449000] adding scores\n",
      "[2017-01-17 20:26:50.481000] fitting\n",
      "[2017-01-17 20:27:25.446000] outputting\n",
      "name = model_100_ss_th_ss_phi_93, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 20:27:28.297000] processing model 94 / 26\n",
      "[2017-01-17 20:27:28.298000] creating model\n",
      "[2017-01-17 20:27:29.671000] adding scores\n",
      "[2017-01-17 20:27:29.705000] fitting\n",
      "[2017-01-17 20:28:03.392000] outputting\n",
      "name = model_100_ss_th_ss_phi_94, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 20:28:09.343000] processing model 95 / 26\n",
      "[2017-01-17 20:28:09.343000] creating model\n",
      "[2017-01-17 20:28:10.696000] adding scores\n",
      "[2017-01-17 20:28:10.730000] fitting\n",
      "[2017-01-17 20:28:49.743000] outputting\n",
      "name = model_100_ss_th_ss_phi_95, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 20:29:13.610000] processing model 96 / 26\n",
      "[2017-01-17 20:29:13.610000] creating model\n",
      "[2017-01-17 20:29:15.052000] adding scores\n",
      "[2017-01-17 20:29:15.087000] fitting\n",
      "[2017-01-17 20:29:50.477000] outputting\n",
      "name = model_100_ss_th_ss_phi_96, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 20:30:09.350000] processing model 97 / 26\n",
      "[2017-01-17 20:30:09.350000] creating model\n",
      "[2017-01-17 20:30:10.706000] adding scores\n",
      "[2017-01-17 20:30:10.742000] fitting\n",
      "[2017-01-17 20:30:46.378000] outputting\n",
      "name = model_100_ss_th_ss_phi_97, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 20:31:03.541000] processing model 98 / 26\n",
      "[2017-01-17 20:31:03.542000] creating model\n",
      "[2017-01-17 20:31:04.971000] adding scores\n",
      "[2017-01-17 20:31:05.011000] fitting\n",
      "[2017-01-17 20:31:40.373000] outputting\n",
      "name = model_100_ss_th_ss_phi_98, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 20:31:48.680000] processing model 99 / 26\n",
      "[2017-01-17 20:31:48.680000] creating model\n",
      "[2017-01-17 20:31:50.115000] adding scores\n",
      "[2017-01-17 20:31:50.149000] fitting\n",
      "[2017-01-17 20:32:28.013000] outputting\n",
      "name = model_100_ss_th_ss_phi_99, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 20:32:45.815000] processing model 100 / 26\n",
      "[2017-01-17 20:32:45.815000] creating model\n",
      "[2017-01-17 20:32:47.300000] adding scores\n",
      "[2017-01-17 20:32:47.334000] fitting\n",
      "[2017-01-17 20:33:23.133000] outputting\n",
      "name = model_100_ss_th_ss_phi_100, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 20:33:25.245000] processing model 101 / 26\n",
      "[2017-01-17 20:33:25.246000] creating model\n",
      "[2017-01-17 20:33:26.610000] adding scores\n",
      "[2017-01-17 20:33:26.651000] fitting\n",
      "[2017-01-17 20:34:04.873000] outputting\n",
      "name = model_100_ss_th_ss_phi_101, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 20:34:07.029000] processing model 102 / 26\n",
      "[2017-01-17 20:34:07.030000] creating model\n",
      "[2017-01-17 20:34:08.387000] adding scores\n",
      "[2017-01-17 20:34:08.422000] fitting\n",
      "[2017-01-17 20:34:43.348000] outputting\n",
      "name = model_100_ss_th_ss_phi_102, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 20:34:45.537000] processing model 103 / 26\n",
      "[2017-01-17 20:34:45.537000] creating model\n",
      "[2017-01-17 20:34:46.987000] adding scores\n",
      "[2017-01-17 20:34:47.020000] fitting\n",
      "[2017-01-17 20:35:22.071000] outputting\n",
      "name = model_100_ss_th_ss_phi_103, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 0.1\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 20:35:24.933000] processing model 104 / 26\n",
      "[2017-01-17 20:35:24.933000] creating model\n",
      "[2017-01-17 20:35:26.292000] adding scores\n",
      "[2017-01-17 20:35:26.326000] fitting\n",
      "[2017-01-17 20:35:56.744000] outputting\n",
      "name = model_100_ss_th_ss_phi_104, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 20:35:58.743000] processing model 105 / 26\n",
      "[2017-01-17 20:35:58.743000] creating model\n",
      "[2017-01-17 20:36:00.182000] adding scores\n",
      "[2017-01-17 20:36:00.216000] fitting\n",
      "[2017-01-17 20:36:30.285000] outputting\n",
      "name = model_100_ss_th_ss_phi_105, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 20:36:32.320000] processing model 106 / 26\n",
      "[2017-01-17 20:36:32.320000] creating model\n",
      "[2017-01-17 20:36:33.771000] adding scores\n",
      "[2017-01-17 20:36:33.807000] fitting\n",
      "[2017-01-17 20:37:07.388000] outputting\n",
      "name = model_100_ss_th_ss_phi_106, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 20:37:09.819000] processing model 107 / 26\n",
      "[2017-01-17 20:37:09.820000] creating model\n",
      "[2017-01-17 20:37:11.194000] adding scores\n",
      "[2017-01-17 20:37:11.233000] fitting\n",
      "[2017-01-17 20:37:47.388000] outputting\n",
      "name = model_100_ss_th_ss_phi_107, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 20:37:51.679000] processing model 108 / 26\n",
      "[2017-01-17 20:37:51.679000] creating model\n",
      "[2017-01-17 20:37:53.044000] adding scores\n",
      "[2017-01-17 20:37:53.077000] fitting\n",
      "[2017-01-17 20:38:29.912000] outputting\n",
      "name = model_100_ss_th_ss_phi_108, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 20:38:44.049000] processing model 109 / 26\n",
      "[2017-01-17 20:38:44.050000] creating model\n",
      "[2017-01-17 20:38:45.634000] adding scores\n",
      "[2017-01-17 20:38:45.666000] fitting\n",
      "[2017-01-17 20:39:22.393000] outputting\n",
      "name = model_100_ss_th_ss_phi_109, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 20:39:29.614000] processing model 110 / 26\n",
      "[2017-01-17 20:39:29.614000] creating model\n",
      "[2017-01-17 20:39:31.002000] adding scores\n",
      "[2017-01-17 20:39:31.042000] fitting\n",
      "[2017-01-17 20:40:06.471000] outputting\n",
      "name = model_100_ss_th_ss_phi_110, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 20:40:12.995000] processing model 111 / 26\n",
      "[2017-01-17 20:40:12.996000] creating model\n",
      "[2017-01-17 20:40:14.367000] adding scores\n",
      "[2017-01-17 20:40:14.399000] fitting\n",
      "[2017-01-17 20:40:50.121000] outputting\n",
      "name = model_100_ss_th_ss_phi_111, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 20:40:52.732000] processing model 112 / 26\n",
      "[2017-01-17 20:40:52.733000] creating model\n",
      "[2017-01-17 20:40:54.090000] adding scores\n",
      "[2017-01-17 20:40:54.130000] fitting\n",
      "[2017-01-17 20:41:33.070000] outputting\n",
      "name = model_100_ss_th_ss_phi_112, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 20:41:35.241000] processing model 113 / 26\n",
      "[2017-01-17 20:41:35.242000] creating model\n",
      "[2017-01-17 20:41:36.538000] adding scores\n",
      "[2017-01-17 20:41:36.567000] fitting\n",
      "[2017-01-17 20:42:15.017000] outputting\n",
      "name = model_100_ss_th_ss_phi_113, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 20:42:17.151000] processing model 114 / 26\n",
      "[2017-01-17 20:42:17.152000] creating model\n",
      "[2017-01-17 20:42:18.528000] adding scores\n",
      "[2017-01-17 20:42:18.561000] fitting\n",
      "[2017-01-17 20:42:56.089000] outputting\n",
      "name = model_100_ss_th_ss_phi_114, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 20:42:58.390000] processing model 115 / 26\n",
      "[2017-01-17 20:42:58.391000] creating model\n",
      "[2017-01-17 20:42:59.785000] adding scores\n",
      "[2017-01-17 20:42:59.816000] fitting\n",
      "[2017-01-17 20:43:38.012000] outputting\n",
      "name = model_100_ss_th_ss_phi_115, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 20:43:40.060000] processing model 116 / 26\n",
      "[2017-01-17 20:43:40.060000] creating model\n",
      "[2017-01-17 20:43:41.511000] adding scores\n",
      "[2017-01-17 20:43:41.544000] fitting\n",
      "[2017-01-17 20:44:19.155000] outputting\n",
      "name = model_100_ss_th_ss_phi_116, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 20:44:21.261000] processing model 117 / 26\n",
      "[2017-01-17 20:44:21.261000] creating model\n",
      "[2017-01-17 20:44:22.613000] adding scores\n",
      "[2017-01-17 20:44:22.650000] fitting\n",
      "[2017-01-17 20:44:53.098000] outputting\n",
      "name = model_100_ss_th_ss_phi_117, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 20:44:55.190000] processing model 118 / 26\n",
      "[2017-01-17 20:44:55.190000] creating model\n",
      "[2017-01-17 20:44:56.585000] adding scores\n",
      "[2017-01-17 20:44:56.617000] fitting\n",
      "[2017-01-17 20:45:28.459000] outputting\n",
      "name = model_100_ss_th_ss_phi_118, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 20:45:30.477000] processing model 119 / 26\n",
      "[2017-01-17 20:45:30.477000] creating model\n",
      "[2017-01-17 20:45:31.826000] adding scores\n",
      "[2017-01-17 20:45:31.865000] fitting\n",
      "[2017-01-17 20:46:05.509000] outputting\n",
      "name = model_100_ss_th_ss_phi_119, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 20:46:09.523000] processing model 120 / 26\n",
      "[2017-01-17 20:46:09.524000] creating model\n",
      "[2017-01-17 20:46:10.916000] adding scores\n",
      "[2017-01-17 20:46:10.951000] fitting\n",
      "[2017-01-17 20:46:44.586000] outputting\n",
      "name = model_100_ss_th_ss_phi_120, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 20:46:48.002000] processing model 121 / 26\n",
      "[2017-01-17 20:46:48.003000] creating model\n",
      "[2017-01-17 20:46:49.436000] adding scores\n",
      "[2017-01-17 20:46:49.466000] fitting\n",
      "[2017-01-17 20:47:26.058000] outputting\n",
      "name = model_100_ss_th_ss_phi_121, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 20:47:32.564000] processing model 122 / 26\n",
      "[2017-01-17 20:47:32.564000] creating model\n",
      "[2017-01-17 20:47:33.921000] adding scores\n",
      "[2017-01-17 20:47:33.954000] fitting\n",
      "[2017-01-17 20:48:15.836000] outputting\n",
      "name = model_100_ss_th_ss_phi_122, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 20:48:18.987000] processing model 123 / 26\n",
      "[2017-01-17 20:48:18.987000] creating model\n",
      "[2017-01-17 20:48:20.321000] adding scores\n",
      "[2017-01-17 20:48:20.352000] fitting\n",
      "[2017-01-17 20:48:58.560000] outputting\n",
      "name = model_100_ss_th_ss_phi_123, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 20:49:00.753000] processing model 124 / 26\n",
      "[2017-01-17 20:49:00.754000] creating model\n",
      "[2017-01-17 20:49:02.214000] adding scores\n",
      "[2017-01-17 20:49:02.253000] fitting\n",
      "[2017-01-17 20:49:38.442000] outputting\n",
      "name = model_100_ss_th_ss_phi_124, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 20:49:40.686000] processing model 125 / 26\n",
      "[2017-01-17 20:49:40.686000] creating model\n",
      "[2017-01-17 20:49:42.184000] adding scores\n",
      "[2017-01-17 20:49:42.222000] fitting\n",
      "[2017-01-17 20:50:20.292000] outputting\n",
      "name = model_100_ss_th_ss_phi_125, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 20:50:22.381000] processing model 126 / 26\n",
      "[2017-01-17 20:50:22.381000] creating model\n",
      "[2017-01-17 20:50:23.761000] adding scores\n",
      "[2017-01-17 20:50:23.796000] fitting\n",
      "[2017-01-17 20:51:00.548000] outputting\n",
      "name = model_100_ss_th_ss_phi_126, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 20:51:02.529000] processing model 127 / 26\n",
      "[2017-01-17 20:51:02.529000] creating model\n",
      "[2017-01-17 20:51:03.959000] adding scores\n",
      "[2017-01-17 20:51:03.992000] fitting\n",
      "[2017-01-17 20:51:40.106000] outputting\n",
      "name = model_100_ss_th_ss_phi_127, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 20:51:42.108000] processing model 128 / 26\n",
      "[2017-01-17 20:51:42.109000] creating model\n",
      "[2017-01-17 20:51:43.556000] adding scores\n",
      "[2017-01-17 20:51:43.590000] fitting\n",
      "[2017-01-17 20:52:20.294000] outputting\n",
      "name = model_100_ss_th_ss_phi_128, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 20:52:22.342000] processing model 129 / 26\n",
      "[2017-01-17 20:52:22.343000] creating model\n",
      "[2017-01-17 20:52:23.690000] adding scores\n",
      "[2017-01-17 20:52:23.729000] fitting\n",
      "[2017-01-17 20:53:00.583000] outputting\n",
      "name = model_100_ss_th_ss_phi_129, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 20:53:02.610000] processing model 130 / 26\n",
      "[2017-01-17 20:53:02.610000] creating model\n",
      "[2017-01-17 20:53:04.061000] adding scores\n",
      "[2017-01-17 20:53:04.093000] fitting\n",
      "[2017-01-17 20:53:34.723000] outputting\n",
      "name = model_100_ss_th_ss_phi_130, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 20:53:36.753000] processing model 131 / 26\n",
      "[2017-01-17 20:53:36.754000] creating model\n",
      "[2017-01-17 20:53:38.109000] adding scores\n",
      "[2017-01-17 20:53:38.147000] fitting\n",
      "[2017-01-17 20:54:10.048000] outputting\n",
      "name = model_100_ss_th_ss_phi_131, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 20:54:12.174000] processing model 132 / 26\n",
      "[2017-01-17 20:54:12.174000] creating model\n",
      "[2017-01-17 20:54:13.540000] adding scores\n",
      "[2017-01-17 20:54:13.573000] fitting\n",
      "[2017-01-17 20:54:49.121000] outputting\n",
      "name = model_100_ss_th_ss_phi_132, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 20:54:51.478000] processing model 133 / 26\n",
      "[2017-01-17 20:54:51.478000] creating model\n",
      "[2017-01-17 20:54:52.795000] adding scores\n",
      "[2017-01-17 20:54:52.826000] fitting\n",
      "[2017-01-17 20:55:27.277000] outputting\n",
      "name = model_100_ss_th_ss_phi_133, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 20:55:30.630000] processing model 134 / 26\n",
      "[2017-01-17 20:55:30.630000] creating model\n",
      "[2017-01-17 20:55:32.082000] adding scores\n",
      "[2017-01-17 20:55:32.116000] fitting\n",
      "[2017-01-17 20:56:08.466000] outputting\n",
      "name = model_100_ss_th_ss_phi_134, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 20:56:14.700000] processing model 135 / 26\n",
      "[2017-01-17 20:56:14.701000] creating model\n",
      "[2017-01-17 20:56:15.978000] adding scores\n",
      "[2017-01-17 20:56:16.009000] fitting\n",
      "[2017-01-17 20:56:52.703000] outputting\n",
      "name = model_100_ss_th_ss_phi_135, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 20:56:54.741000] processing model 136 / 26\n",
      "[2017-01-17 20:56:54.742000] creating model\n",
      "[2017-01-17 20:56:56.209000] adding scores\n",
      "[2017-01-17 20:56:56.242000] fitting\n",
      "[2017-01-17 20:57:32.450000] outputting\n",
      "name = model_100_ss_th_ss_phi_136, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 20:57:35.328000] processing model 137 / 26\n",
      "[2017-01-17 20:57:35.329000] creating model\n",
      "[2017-01-17 20:57:36.718000] adding scores\n",
      "[2017-01-17 20:57:36.749000] fitting\n",
      "[2017-01-17 20:58:12.264000] outputting\n",
      "name = model_100_ss_th_ss_phi_137, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 20:58:14.267000] processing model 138 / 26\n",
      "[2017-01-17 20:58:14.267000] creating model\n",
      "[2017-01-17 20:58:15.541000] adding scores\n",
      "[2017-01-17 20:58:15.571000] fitting\n",
      "[2017-01-17 20:58:53.898000] outputting\n",
      "name = model_100_ss_th_ss_phi_138, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 20:58:55.980000] processing model 139 / 26\n",
      "[2017-01-17 20:58:55.980000] creating model\n",
      "[2017-01-17 20:58:57.450000] adding scores\n",
      "[2017-01-17 20:58:57.483000] fitting\n",
      "[2017-01-17 20:59:34.613000] outputting\n",
      "name = model_100_ss_th_ss_phi_139, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 20:59:36.702000] processing model 140 / 26\n",
      "[2017-01-17 20:59:36.702000] creating model\n",
      "[2017-01-17 20:59:38.057000] adding scores\n",
      "[2017-01-17 20:59:38.091000] fitting\n",
      "[2017-01-17 21:00:14.737000] outputting\n",
      "name = model_100_ss_th_ss_phi_140, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 21:00:16.846000] processing model 141 / 26\n",
      "[2017-01-17 21:00:16.846000] creating model\n",
      "[2017-01-17 21:00:18.277000] adding scores\n",
      "[2017-01-17 21:00:18.317000] fitting\n",
      "[2017-01-17 21:00:57.015000] outputting\n",
      "name = model_100_ss_th_ss_phi_141, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 21:00:59.431000] processing model 142 / 26\n",
      "[2017-01-17 21:00:59.431000] creating model\n",
      "[2017-01-17 21:01:00.980000] adding scores\n",
      "[2017-01-17 21:01:01.012000] fitting\n",
      "[2017-01-17 21:01:38.626000] outputting\n",
      "name = model_100_ss_th_ss_phi_142, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 1000.0\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 21:01:40.720000] processing model 143 / 26\n",
      "[2017-01-17 21:01:40.721000] creating model\n",
      "[2017-01-17 21:01:42.105000] adding scores\n",
      "[2017-01-17 21:01:42.139000] fitting\n",
      "[2017-01-17 21:02:14.801000] outputting\n",
      "name = model_100_ss_th_ss_phi_143, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 21:02:17.011000] processing model 144 / 26\n",
      "[2017-01-17 21:02:17.012000] creating model\n",
      "[2017-01-17 21:02:18.534000] adding scores\n",
      "[2017-01-17 21:02:18.566000] fitting\n",
      "[2017-01-17 21:02:51.432000] outputting\n",
      "name = model_100_ss_th_ss_phi_144, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 21:02:55.456000] processing model 145 / 26\n",
      "[2017-01-17 21:02:55.457000] creating model\n",
      "[2017-01-17 21:02:56.937000] adding scores\n",
      "[2017-01-17 21:02:56.976000] fitting\n",
      "[2017-01-17 21:03:34.193000] outputting\n",
      "name = model_100_ss_th_ss_phi_145, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 21:03:36.663000] processing model 146 / 26\n",
      "[2017-01-17 21:03:36.664000] creating model\n",
      "[2017-01-17 21:03:38.201000] adding scores\n",
      "[2017-01-17 21:03:38.234000] fitting\n",
      "[2017-01-17 21:04:15.327000] outputting\n",
      "name = model_100_ss_th_ss_phi_146, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 21:04:18.736000] processing model 147 / 26\n",
      "[2017-01-17 21:04:18.736000] creating model\n",
      "[2017-01-17 21:04:20.234000] adding scores\n",
      "[2017-01-17 21:04:20.271000] fitting\n",
      "[2017-01-17 21:04:58.466000] outputting\n",
      "name = model_100_ss_th_ss_phi_147, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 21:05:04.779000] processing model 148 / 26\n",
      "[2017-01-17 21:05:04.779000] creating model\n",
      "[2017-01-17 21:05:06.125000] adding scores\n",
      "[2017-01-17 21:05:06.157000] fitting\n",
      "[2017-01-17 21:05:42.561000] outputting\n",
      "name = model_100_ss_th_ss_phi_148, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 21:05:44.786000] processing model 149 / 26\n",
      "[2017-01-17 21:05:44.787000] creating model\n",
      "[2017-01-17 21:05:46.308000] adding scores\n",
      "[2017-01-17 21:05:46.339000] fitting\n",
      "[2017-01-17 21:06:24.016000] outputting\n",
      "name = model_100_ss_th_ss_phi_149, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 21:06:27.846000] processing model 150 / 26\n",
      "[2017-01-17 21:06:27.847000] creating model\n",
      "[2017-01-17 21:06:29.205000] adding scores\n",
      "[2017-01-17 21:06:29.240000] fitting\n",
      "[2017-01-17 21:07:05.779000] outputting\n",
      "name = model_100_ss_th_ss_phi_150, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 21:07:09.764000] processing model 151 / 26\n",
      "[2017-01-17 21:07:09.765000] creating model\n",
      "[2017-01-17 21:07:11.226000] adding scores\n",
      "[2017-01-17 21:07:11.257000] fitting\n",
      "[2017-01-17 21:07:46.396000] outputting\n",
      "name = model_100_ss_th_ss_phi_151, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 21:07:48.498000] processing model 152 / 26\n",
      "[2017-01-17 21:07:48.499000] creating model\n",
      "[2017-01-17 21:07:49.865000] adding scores\n",
      "[2017-01-17 21:07:49.900000] fitting\n",
      "[2017-01-17 21:08:28.232000] outputting\n",
      "name = model_100_ss_th_ss_phi_152, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 21:08:30.335000] processing model 153 / 26\n",
      "[2017-01-17 21:08:30.336000] creating model\n",
      "[2017-01-17 21:08:31.771000] adding scores\n",
      "[2017-01-17 21:08:31.805000] fitting\n",
      "[2017-01-17 21:09:08.704000] outputting\n",
      "name = model_100_ss_th_ss_phi_153, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 21:09:10.819000] processing model 154 / 26\n",
      "[2017-01-17 21:09:10.820000] creating model\n",
      "[2017-01-17 21:09:12.264000] adding scores\n",
      "[2017-01-17 21:09:12.301000] fitting\n",
      "[2017-01-17 21:09:50.372000] outputting\n",
      "name = model_100_ss_th_ss_phi_154, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 21:09:52.474000] processing model 155 / 26\n",
      "[2017-01-17 21:09:52.474000] creating model\n",
      "[2017-01-17 21:09:53.823000] adding scores\n",
      "[2017-01-17 21:09:53.858000] fitting\n",
      "[2017-01-17 21:10:29.153000] outputting\n",
      "name = model_100_ss_th_ss_phi_155, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 100000.0\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n",
      "[2017-01-17 21:10:31.442000] processing model 156 / 26\n",
      "[2017-01-17 21:10:31.443000] creating model\n",
      "[2017-01-17 21:10:32.892000] adding scores\n",
      "[2017-01-17 21:10:32.929000] fitting\n",
      "[2017-01-17 21:11:04.916000] outputting\n",
      "name = model_100_ss_th_ss_phi_156, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -100000.0\n",
      "\n",
      "[2017-01-17 21:11:07.034000] processing model 157 / 26\n",
      "[2017-01-17 21:11:07.034000] creating model\n",
      "[2017-01-17 21:11:08.394000] adding scores\n",
      "[2017-01-17 21:11:08.426000] fitting\n",
      "[2017-01-17 21:11:39.836000] outputting\n",
      "name = model_100_ss_th_ss_phi_157, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -1000.0\n",
      "\n",
      "[2017-01-17 21:11:41.958000] processing model 158 / 26\n",
      "[2017-01-17 21:11:41.958000] creating model\n",
      "[2017-01-17 21:11:43.388000] adding scores\n",
      "[2017-01-17 21:11:43.421000] fitting\n",
      "[2017-01-17 21:12:18.876000] outputting\n",
      "name = model_100_ss_th_ss_phi_158, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -10.0\n",
      "\n",
      "[2017-01-17 21:12:21.369000] processing model 159 / 26\n",
      "[2017-01-17 21:12:21.370000] creating model\n",
      "[2017-01-17 21:12:22.734000] adding scores\n",
      "[2017-01-17 21:12:22.767000] fitting\n",
      "[2017-01-17 21:12:56.793000] outputting\n",
      "name = model_100_ss_th_ss_phi_159, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n",
      "[2017-01-17 21:13:00.208000] processing model 160 / 26\n",
      "[2017-01-17 21:13:00.209000] creating model\n",
      "[2017-01-17 21:13:01.555000] adding scores\n",
      "[2017-01-17 21:13:01.585000] fitting\n",
      "[2017-01-17 21:13:38.039000] outputting\n",
      "name = model_100_ss_th_ss_phi_160, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -0.1\n",
      "\n",
      "[2017-01-17 21:13:44.625000] processing model 161 / 26\n",
      "[2017-01-17 21:13:44.625000] creating model\n",
      "[2017-01-17 21:13:46.087000] adding scores\n",
      "[2017-01-17 21:13:46.123000] fitting\n",
      "[2017-01-17 21:14:29.244000] outputting\n",
      "name = model_100_ss_th_ss_phi_161, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -0.01\n",
      "\n",
      "[2017-01-17 21:14:31.492000] processing model 162 / 26\n",
      "[2017-01-17 21:14:31.492000] creating model\n",
      "[2017-01-17 21:14:32.824000] adding scores\n",
      "[2017-01-17 21:14:32.855000] fitting\n",
      "[2017-01-17 21:15:16.652000] outputting\n",
      "name = model_100_ss_th_ss_phi_162, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -1e-05\n",
      "\n",
      "[2017-01-17 21:15:19.266000] processing model 163 / 26\n",
      "[2017-01-17 21:15:19.266000] creating model\n",
      "[2017-01-17 21:15:20.821000] adding scores\n",
      "[2017-01-17 21:15:20.856000] fitting\n",
      "[2017-01-17 21:16:03.198000] outputting\n",
      "name = model_100_ss_th_ss_phi_163, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = 0.1\n",
      "\n",
      "[2017-01-17 21:16:05.601000] processing model 164 / 26\n",
      "[2017-01-17 21:16:05.602000] creating model\n",
      "[2017-01-17 21:16:07.421000] adding scores\n",
      "[2017-01-17 21:16:07.456000] fitting\n",
      "[2017-01-17 21:17:04.136000] outputting\n",
      "name = model_100_ss_th_ss_phi_164, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = 10\n",
      "\n",
      "[2017-01-17 21:17:07.008000] processing model 165 / 26\n",
      "[2017-01-17 21:17:07.008000] creating model\n",
      "[2017-01-17 21:17:08.647000] adding scores\n",
      "[2017-01-17 21:17:08.689000] fitting\n",
      "[2017-01-17 21:17:56.598000] outputting\n",
      "name = model_100_ss_th_ss_phi_165, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = 100\n",
      "\n",
      "[2017-01-17 21:17:59.085000] processing model 166 / 26\n",
      "[2017-01-17 21:17:59.085000] creating model\n",
      "[2017-01-17 21:18:00.677000] adding scores\n",
      "[2017-01-17 21:18:00.719000] fitting\n",
      "[2017-01-17 21:18:42.351000] outputting\n",
      "name = model_100_ss_th_ss_phi_166, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = 1000.0\n",
      "\n",
      "[2017-01-17 21:18:44.507000] processing model 167 / 26\n",
      "[2017-01-17 21:18:44.509000] creating model\n",
      "[2017-01-17 21:18:46.220000] adding scores\n",
      "[2017-01-17 21:18:46.265000] fitting\n",
      "[2017-01-17 21:19:25.822000] outputting\n",
      "name = model_100_ss_th_ss_phi_167, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = 100000.0\n",
      "\n",
      "[2017-01-17 21:19:27.959000] processing model 168 / 26\n",
      "[2017-01-17 21:19:27.960000] creating model\n",
      "[2017-01-17 21:19:29.461000] adding scores\n",
      "[2017-01-17 21:19:29.494000] fitting\n",
      "[2017-01-17 21:20:07.775000] outputting\n",
      "name = model_100_ss_th_ss_phi_168, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = 10000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taus_th = [-1e+5, -1e+3, -1e+1, -1, -1e-1, -1e-2, -1e-5, 1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "taus_phi = [-1e+5, -1e+3, -1e+1, -1, -1e-1, -1e-2, -1e-5, 1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "for idx_th, t_th in enumerate(taus_th):\n",
    "    for idx_phi, t_phi in enumerate(taus_phi):\n",
    "        print '[{}] processing model {} / {}'.format(datetime.now(), idx_th * len(taus_th) + idx_phi, len(taus_th) + len(taus_phi))\n",
    "        tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                                    n_top_tokens=15, p_mass_threshold=0.25)\n",
    "        tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "        tmp_model.regularizers['ss_theta_regularizer'].tau = t_th\n",
    "        tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "        tmp_model.regularizers['ss_phi_regularizer'].tau = t_phi\n",
    "        tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_ss_th_ss_phi_{}'.format(idx_th * len(taus_th) + idx_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taus_th = [-1e+5, -1e+3, -1e+1, -1, -1e-1, -1e-2, -1e-5, 1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "taus_phi = [-1e+5, -1e+3, -1e+1, -1, -1e-1, -1e-2, -1e-5, 1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "taus_decor_th = [1e-1, 10, 100, 1e+3, 1e+5, 1e+7]\n",
    "for idx_decor_th, t_decor_th in enumerate(taus_decor_th):\n",
    "    for idx_th, t_th in enumerate(taus_th):\n",
    "        for idx_phi, t_phi in enumerate(taus_phi):\n",
    "            print '[{}] processing model {} / {}'.format(datetime.now(), idx_th + idx_phi, len(taus_th) + len(taus_phi))\n",
    "            tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                                        n_top_tokens=15, p_mass_threshold=0.25)\n",
    "            tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "            tmp_model.regularizers['ss_theta_regularizer'].tau = t_th\n",
    "            tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "            tmp_model.regularizers['ss_phi_regularizer'].tau = t_phi\n",
    "            tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "            tmp_model.regularizers['decorrelator_phi_regularizer'].tau = t_decor_th\n",
    "            tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_ss_th_ss_phi_decor_{}'.format(idx_th + idx_phi + idx+_decor_th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 13:50:14.199000] creating model\n",
      "[2017-01-18 13:50:15.808000] adding scores\n",
      "[2017-01-18 13:50:15.828000] fitting\n",
      "[2017-01-18 13:50:58.132000] outputting\n",
      "name = model_100_2, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_2')\n",
    "model2 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 13:51:44.164000] creating model\n",
      "[2017-01-18 13:51:45.670000] adding scores\n",
      "[2017-01-18 13:51:45.681000] fitting\n",
      "[2017-01-18 13:52:20.233000] outputting\n",
      "name = model_100_3, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:10:41.023000] creating model\n",
      "[2017-01-18 14:10:42.603000] adding scores\n",
      "[2017-01-18 14:10:42.640000] fitting\n",
      "[2017-01-18 14:11:17.749000] outputting\n",
      "name = model_100_4, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1.5\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:13:34.646000] creating model\n",
      "[2017-01-18 14:13:36.221000] adding scores\n",
      "[2017-01-18 14:13:36.268000] fitting\n",
      "[2017-01-18 14:14:11.615000] outputting\n",
      "name = model_100_5, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -2\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_5')\n",
    "# BAD TOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:15:42.702000] creating model\n",
      "[2017-01-18 14:15:44.216000] adding scores\n",
      "[2017-01-18 14:15:44.255000] fitting\n",
      "[2017-01-18 14:16:19.501000] outputting\n",
      "name = model_100_6, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:18:07.942000] creating model\n",
      "[2017-01-18 14:18:09.529000] adding scores\n",
      "[2017-01-18 14:18:09.582000] fitting\n",
      "[2017-01-18 14:18:45.054000] outputting\n",
      "name = model_100_7, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_7')\n",
    "# THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:22:17.846000] creating model\n",
      "[2017-01-18 14:22:19.331000] adding scores\n",
      "[2017-01-18 14:22:19.370000] fitting\n",
      "[2017-01-18 14:22:54.246000] outputting\n",
      "name = model_100_8, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top words better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:24:55.615000] creating model\n",
      "[2017-01-18 14:24:57.161000] adding scores\n",
      "[2017-01-18 14:24:57.201000] fitting\n",
      "[2017-01-18 14:25:32.289000] outputting\n",
      "name = model_100_9, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 5\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 5\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:33:32.630000] creating model\n",
      "[2017-01-18 14:33:33.978000] adding scores\n",
      "[2017-01-18 14:33:33.992000] fitting\n",
      "[2017-01-18 14:34:12.437000] outputting\n",
      "name = model_100_10, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:35:42.327000] creating model\n",
      "[2017-01-18 14:35:43.693000] adding scores\n",
      "[2017-01-18 14:35:43.739000] fitting\n",
      "[2017-01-18 14:36:18.083000] outputting\n",
      "name = model_100_11, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:38:42.500000] creating model\n",
      "[2017-01-18 14:38:44.198000] adding scores\n",
      "[2017-01-18 14:38:44.287000] fitting\n",
      "[2017-01-18 14:39:17.185000] outputting\n",
      "name = model_100_12, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_12')\n",
    "# THIS ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:41:00.920000] creating model\n",
      "[2017-01-18 14:41:02.436000] adding scores\n",
      "[2017-01-18 14:41:02.470000] fitting\n",
      "[2017-01-18 14:41:37.578000] outputting\n",
      "name = model_100_13, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:41:43.035000] creating model\n",
      "[2017-01-18 14:41:44.697000] adding scores\n",
      "[2017-01-18 14:41:44.741000] fitting\n",
      "[2017-01-18 14:42:20.007000] outputting\n",
      "name = model_100_14, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -3\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2017-01-18 14:43:35.376000] creating model\n",
      "[2017-01-18 14:43:36.783000] adding scores\n",
      "[2017-01-18 14:43:36.825000] fitting\n",
      "[2017-01-18 14:44:08.948000] outputting\n",
      "name = model_100_15, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2.5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=20, _model_name='model_100_15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем 3 модели: без регуляриз,  7 (норм, но у ядра размер 90) и 12 (норм и ядро размера 24). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
