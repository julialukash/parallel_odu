{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import artm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "\n",
    "import distances_helper as dh \n",
    "import create_model_helper as cmh\n",
    "\n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\UCI_filtered_ngramm_trimmed_without_names\\np_28_02\\models.txt\n"
     ]
    }
   ],
   "source": [
    "config = ConfigPaths('config.cfg')\n",
    "plot_maker = PlotMaker()\n",
    "print config.models_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_file = open(config.models_file_name, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_pickle_file(dists, filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'wb')\n",
    "    pickle.dump(dists, pickle_file)\n",
    "    pickle_file.close()\n",
    "def load_pickle_file(filename):\n",
    "    pickle_filename = path.join(config.experiment_path, filename)\n",
    "    pickle_file = open(pickle_filename, 'rb')\n",
    "    p_file = pickle.load(pickle_file)\n",
    "    pickle_file.close()\n",
    "    return p_file\n",
    "def save_model_pickle(_model_name, _model, _save=True):\n",
    "    phi = _model.get_phi()\n",
    "    phi = phi[(phi.T != 0).any()]\n",
    "    theta = _model.get_theta()    \n",
    "    saved_top_tokens = _model.score_tracker['top_tokens_score'].last_tokens\n",
    "    if _save:\n",
    "        save_pickle_file(phi, 'phi_{}.p'.format(_model_name))\n",
    "        save_pickle_file(theta, 'theta_{}.p'.format(_model_name))\n",
    "        save_pickle_file(saved_top_tokens, 'saved_top_tokens_{}.p'.format(_model_name))\n",
    "    return phi, theta, saved_top_tokens\n",
    "def load_model_pickle(_model_name, _distance_name):\n",
    "    phi = load_pickle_file('phi_{}.p'.format(_model_name))\n",
    "    theta = load_pickle_file('theta_{}.p'.format(_model_name))\n",
    "    saved_top_tokens = load_pickle_file('saved_top_tokens_{}.p'.format(_model_name))\n",
    "    distances = load_pickle_file('{}.p'.format(_distance_name))\n",
    "    return phi, theta, saved_top_tokens, distances\n",
    "# roman:  В каждой строчке Θ находим максимальное значение и в его столб-це остальных зануляем, а его делаем 1\n",
    "def make_theta_for_single_decomposition(theta):\n",
    "    theta_new = theta.copy()\n",
    "    for row_idx, (row_name, row) in enumerate(theta_new.iterrows()):\n",
    "        max_idx = row.idxmax()\n",
    "        theta_new.iloc[:, max_idx] = 0\n",
    "        theta_new.iloc[row_idx, max_idx] = 1\n",
    "    return theta_new\n",
    "# roman: В каждом столбце матрицы Φ зануляем n_topics − 1 самое маленькое значение\n",
    "def make_phi_for_single_decomposition(phi, n_topics=None):\n",
    "    if n_topics is None:\n",
    "        n_topics = phi.shape[1]\n",
    "    phi_new = phi.copy()\n",
    "    for col_idx, (col_name, col) in enumerate(phi_new.iteritems()):\n",
    "        th = col.sort_values()[n_topics] \n",
    "        col[col < th] = 0\n",
    "        phi_new.iloc[:, col_idx] = phi_new.iloc[:, col_idx] / np.sum(phi_new.iloc[:, col_idx])\n",
    "    return phi_new\n",
    "\n",
    "# A = W x R; Lt={i:W_it != 0}, Rt={j: R_tj != 0}; \n",
    "# Разложение ! <-> не существует двух таких тем t1 и t2: Lt1 --- подмножество Lt2 или Rt1 --- подмножество Rt2\n",
    "def check_theta_single_decomposition(theta, _debug_print=False):\n",
    "    is_subset = lambda list_of_set, x: len([one_set for one_set in list_of_set if x.issubset(one_set) or one_set.issubset(x)]) != 0\n",
    "    non_zero_indices = {}\n",
    "    for row_idx, (_, row) in enumerate(theta.iterrows()):\n",
    "        indices = set(np.where(row != 0)[0])\n",
    "        if not is_subset(non_zero_indices.values(), indices):\n",
    "            non_zero_indices[row_idx] = indices\n",
    "        elif _debug_print:\n",
    "            print 'Incorrect row index =', row_idx\n",
    "    return len(non_zero_indices) == theta.shape[0]\n",
    "def check_phi_single_decomposition(phi, _debug_print=False):\n",
    "    is_subset = lambda list_of_set, x: len([one_set for one_set in list_of_set if x.issubset(one_set) or one_set.issubset(x)]) != 0\n",
    "    non_zero_indices = {}\n",
    "    for col_idx, (_, col) in enumerate(phi.iteritems()):\n",
    "        indices = set(np.where(col != 0)[0])\n",
    "        if not is_subset(non_zero_indices.values(), indices):\n",
    "            non_zero_indices[col_idx] = indices\n",
    "        elif _debug_print:\n",
    "            print 'Incorrect column index =', col_idx\n",
    "    return len(non_zero_indices) == phi.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=config.output_batches_path,\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path=config.dictionary_path + '.dict')\n",
    "n_iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = model_100_1_iter_0, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, top_tokens_score = 15, topic_kernel_score = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25, class_name='ngramm')\n",
    "tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20, \n",
    "                              _model_name='model_100_1_iter_{}'.format(n_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check phi correctness = True, theta correctness = False\n",
      "Check new_phi correctness = True, new_theta correctness = True\n"
     ]
    }
   ],
   "source": [
    "phi, theta = tmp_model.get_phi(), tmp_model.get_theta()\n",
    "phi_new = make_phi_for_single_decomposition(phi)\n",
    "theta_new = make_theta_for_single_decomposition(theta)\n",
    "\n",
    "print 'Check phi correctness = {}, theta correctness = {}' \\\n",
    "        .format(check_phi_single_decomposition(phi), check_theta_single_decomposition(theta))\n",
    "print 'Check new_phi correctness = {}, new_theta correctness = {}' \\\n",
    "        .format(check_phi_single_decomposition(phi_new), check_theta_single_decomposition(theta_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = model_100_7_iter_0, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, top_tokens_score = 15, topic_kernel_score = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25, class_name='ngramm')\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20, \n",
    "                              _model_name='model_100_7_iter_{}'.format(n_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check phi correctness = True, theta correctness = True\n",
      "Check new_phi correctness = True, new_theta correctness = True\n"
     ]
    }
   ],
   "source": [
    "phi, theta = tmp_model.get_phi(), tmp_model.get_theta()\n",
    "phi_new = make_phi_for_single_decomposition(phi)\n",
    "theta_new = make_theta_for_single_decomposition(theta)\n",
    "\n",
    "print 'Check phi correctness = {}, theta correctness = {}' \\\n",
    "        .format(check_phi_single_decomposition(phi), check_theta_single_decomposition(theta))\n",
    "print 'Check new_phi correctness = {}, new_theta correctness = {}' \\\n",
    "        .format(check_phi_single_decomposition(phi_new), check_theta_single_decomposition(theta_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = model_100_12_iter_0, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 20, top_tokens_score = 15, topic_kernel_score = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = cmh.create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25, class_name='ngramm')\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model = cmh.fit_one_model(plot_maker, batch_vectorizer, models_file, config, \n",
    "                              tmp_model, _n_iterations=20, \n",
    "                              _model_name='model_100_12_iter_{}'.format(n_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check phi correctness = True, theta correctness = True\n",
      "Check new_phi correctness = True, new_theta correctness = True\n"
     ]
    }
   ],
   "source": [
    "phi, theta = tmp_model.get_phi(), tmp_model.get_theta()\n",
    "phi_new = make_phi_for_single_decomposition(phi)\n",
    "theta_new = make_theta_for_single_decomposition(theta)\n",
    "\n",
    "print 'Check phi correctness = {}, theta correctness = {}' \\\n",
    "        .format(check_phi_single_decomposition(phi), check_theta_single_decomposition(theta))\n",
    "print 'Check new_phi correctness = {}, new_theta correctness = {}' \\\n",
    "        .format(check_phi_single_decomposition(phi_new), check_theta_single_decomposition(theta_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем 3 модели: без регуляриз,  7 (норм, но у ядра размер 90) и 12 (норм и ядро размера 24). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
