{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import artm\n",
    "print artm.version()\n",
    "\n",
    "from os import path, mkdir\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "sys.path.insert(0, '..\\\\modules\\\\helpers')\n",
    "from plot_helper import PlotMaker\n",
    "from config_helper import ConfigPaths\n",
    "from print_helper import PrintHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = ConfigPaths('config.cfg')\n",
    "plot_maker = PlotMaker()\n",
    "printer = PrintHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\\\\topic_modeling\\\\csi_science_collections.git\\experiments\\UCI_filtered_ngramm_trimmed_without_names\\04_12_num_topics\\models.txt\n"
     ]
    }
   ],
   "source": [
    "print config.models_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_file = open(config.models_file_name, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(current_dictionary, n_topics, n_doc_passes, seed_value, n_top_tokens, p_mass_threshold):    \n",
    "    print '[{}] creating model'.format(datetime.now())\n",
    "    model = artm.ARTM(num_topics=n_topics, dictionary=current_dictionary, cache_theta=True, seed=seed_value, \n",
    "                  class_ids={'ngramm': 1.0, 'author_id': 0.0, 'author': 0.0, \n",
    "                             'post_tag': 0.0, 'projects': 0.0, 'category': 0.0,\n",
    "                             'following_users': 0.0})\n",
    "    model.num_document_passes = n_doc_passes\n",
    "    add_scores_to_model(model, n_top_tokens=n_top_tokens, p_mass_threshold=p_mass_threshold)\n",
    "    return model\n",
    "\n",
    "\n",
    "def add_scores_to_model(artm_model, n_top_tokens, p_mass_threshold):\n",
    "    print '[{}] adding scores'.format(datetime.now())\n",
    "    artm_model.scores.add(artm.PerplexityScore(name='perplexity_score',\n",
    "                                      use_unigram_document_model=False,\n",
    "                                      dictionary=dictionary))\n",
    "    artm_model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score', class_id='ngramm'))\n",
    "    artm_model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))\n",
    "    artm_model.scores.add(artm.TopicKernelScore(name='topic_kernel_score', class_id='ngramm', \n",
    "                                                probability_mass_threshold=p_mass_threshold))\n",
    "    artm_model.scores.add(artm.TopTokensScore(name='top_tokens_score', class_id='ngramm', num_tokens=n_top_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_one_model(dictionary, _n_topics, _n_doc_passes, _seed_value, _n_top_tokens, _p_mass_threshold, _n_iterations,\n",
    "                     _model_name=''):\n",
    "    print '[{}] processing model'.format(datetime.now())\n",
    "    model = create_model(current_dictionary=dictionary, n_topics=_n_topics, n_doc_passes=_n_doc_passes, seed_value=_seed_value,\n",
    "                         n_top_tokens=_n_top_tokens, p_mass_threshold=_p_mass_threshold)\n",
    "    model = fit_one_model(model, _n_iterations, _model_name)\n",
    "    return model\n",
    "    \n",
    "def fit_one_model(model, _n_iterations, _model_name=''): \n",
    "    print '[{}] fitting'.format(datetime.now())\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=_n_iterations)\n",
    "    print '[{}] outputting'.format(datetime.now())\n",
    "    printer.print_artm_model(model, _model_name, _n_iterations, output_file=models_file)\n",
    "    model_pics_file_name =  path.join(config.experiment_path, _model_name)\n",
    "    plot_maker.make_tm_plots(model, model_pics_file_name)\n",
    "    model_output_file_name = path.join(config.experiment_path, _model_name + '.txt')\n",
    "    printer.print_scores(model, _model_name, _n_iterations, model_output_file_name)\n",
    "    printer.print_top_tokens(model, model_output_file_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(_model, _model_name): \n",
    "    print '[{}] saving model'.format(datetime.now())\n",
    "    model_output_file_name = path.join(config.models_archive_path, _model_name)\n",
    "    _model.save(filename=model_output_file_name+'_saved_p_wt', model_name=_model_name+'p_wt')\n",
    "    _model.save(filename=model_output_file_name+'_saved_n_wt', model_name=_model_name+'n_wt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch_vectorizer = artm.BatchVectorizer(data_path=config.dataset_path,\n",
    "#                                         data_format='bow_uci',\n",
    "#                                         collection_name=config.collection_name,\n",
    "#                                         target_folder=config.output_batches_path)\n",
    "# dictionary = artm.Dictionary()\n",
    "# dictionary.gather(data_path=config.output_batches_path,\n",
    "#                   vocab_file_path=config.vocabulary_path)\n",
    "# dictionary.save(dictionary_path=config.dictionary_path)\n",
    "# dictionary.save_text(dictionary_path=config.dictionary_path + '.txt')\n",
    "# dictionary.load_text(dictionary_path=config.dictionary_path + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=config.output_batches_path,\n",
    "                                        data_format='batches')\n",
    "dictionary = artm.Dictionary()\n",
    "dictionary.load(dictionary_path=config.dictionary_path + '.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary.filter(min_tf=5, max_tf=2000, min_df_rate=0.01, max_df_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:25:11.403000] creating model\n",
      "[2016-12-04 23:25:12.741000] adding scores\n",
      "[2016-12-04 23:25:12.746000] fitting\n",
      "[2016-12-04 23:25:25.608000] outputting\n",
      "name = model_10, n_topics = 10, n_doc_passes = 5, seed_value = 100, n_iterations = 25, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=10, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=25, _model_name='model_10')\n",
    "model_10 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:25:55.419000] creating model\n",
      "[2016-12-04 23:25:56.596000] adding scores\n",
      "[2016-12-04 23:25:56.601000] fitting\n",
      "[2016-12-04 23:26:12.144000] outputting\n",
      "name = model_20, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 25, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=25, _model_name='model_20')\n",
    "model_20 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:26:40.875000] creating model\n",
      "[2016-12-04 23:26:42.083000] adding scores\n",
      "[2016-12-04 23:26:42.087000] fitting\n",
      "[2016-12-04 23:27:05.004000] outputting\n",
      "name = model_50, n_topics = 50, n_doc_passes = 5, seed_value = 100, n_iterations = 25, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=50, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=25, _model_name='model_50')\n",
    "model_50 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:27:28.032000] creating model\n",
      "[2016-12-04 23:27:29.436000] adding scores\n",
      "[2016-12-04 23:27:29.440000] fitting\n",
      "[2016-12-04 23:28:05.010000] outputting\n",
      "name = model_100, n_topics = 100, n_doc_passes = 5, seed_value = 100, n_iterations = 25, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=100, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=25, _model_name='model_100')\n",
    "model_100 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:28:27.618000] creating model\n",
      "[2016-12-04 23:28:29.059000] adding scores\n",
      "[2016-12-04 23:28:29.065000] fitting\n",
      "[2016-12-04 23:29:57.543000] outputting\n",
      "name = model_250, n_topics = 250, n_doc_passes = 5, seed_value = 100, n_iterations = 25, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=250, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=25, _model_name='model_250')\n",
    "model_250 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:30:18.706000] creating model\n",
      "[2016-12-04 23:30:20.660000] adding scores\n",
      "[2016-12-04 23:30:20.673000] fitting\n",
      "[2016-12-04 23:33:19.137000] outputting\n",
      "name = model_500, n_topics = 500, n_doc_passes = 5, seed_value = 100, n_iterations = 25, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=500, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=25, _model_name='model_500')\n",
    "model_500 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:33:40.724000] creating model\n",
      "[2016-12-04 23:33:42.804000] adding scores\n",
      "[2016-12-04 23:33:42.816000] fitting\n",
      "[2016-12-04 23:38:23.825000] outputting\n",
      "name = model_750, n_topics = 750, n_doc_passes = 5, seed_value = 100, n_iterations = 25, n_top_tokens = 15, p_threshold = 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=750, n_doc_passes=5, seed_value=100,\n",
    "                            n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=25, _model_name='model_750')\n",
    "model_750 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# модель с маленьким числом тем, будем пытаться разрядить только декорр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:38:52.119000] creating model\n",
      "[2016-12-04 23:38:53.425000] adding scores\n",
      "[2016-12-04 23:38:53.430000] fitting\n",
      "[2016-12-04 23:39:04.745000] outputting\n",
      "name = model_20_decor_reg_1, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 10\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_1')\n",
    "model_20_decor_reg_1 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:39:20.369000] creating model\n",
      "[2016-12-04 23:39:21.630000] adding scores\n",
      "[2016-12-04 23:39:21.636000] fitting\n",
      "[2016-12-04 23:39:33.695000] outputting\n",
      "name = model_20_decor_reg_2, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 100\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_2')\n",
    "model_20_decor_reg_2 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_20_decor_reg_2 не сильно отличается от model_20_decor_reg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:39:48.319000] creating model\n",
      "[2016-12-04 23:39:49.482000] adding scores\n",
      "[2016-12-04 23:39:49.487000] fitting\n",
      "[2016-12-04 23:39:59.498000] outputting\n",
      "name = model_20_decor_reg_3, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1000\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_3')\n",
    "model_20_decor_reg_3 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:40:13.729000] creating model\n",
      "[2016-12-04 23:40:14.910000] adding scores\n",
      "[2016-12-04 23:40:14.926000] fitting\n",
      "[2016-12-04 23:40:25.096000] outputting\n",
      "name = model_20_decor_reg_4, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 100000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+5\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_4')\n",
    "model_20_decor_reg_4 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:40:39.671000] creating model\n",
      "[2016-12-04 23:40:40.839000] adding scores\n",
      "[2016-12-04 23:40:40.844000] fitting\n",
      "[2016-12-04 23:40:52.385000] outputting\n",
      "name = model_20_decor_reg_5, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 10000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+7\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_5')\n",
    "model_20_decor_reg_5 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# много тем смешались"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:41:03.027000] creating model\n",
      "[2016-12-04 23:41:04.172000] adding scores\n",
      "[2016-12-04 23:41:04.177000] fitting\n",
      "[2016-12-04 23:41:15.299000] outputting\n",
      "name = model_20_decor_reg_6, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+9\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_6')\n",
    "model_20_decor_reg_6 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# выродилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:41:16.480000] creating model\n",
      "[2016-12-04 23:41:17.782000] adding scores\n",
      "[2016-12-04 23:41:17.782000] fitting\n",
      "[2016-12-04 23:41:27.475000] outputting\n",
      "name = model_20_decor_reg_7, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 100000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+8\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_7')\n",
    "model_20_decor_reg_7 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# плохая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:41:28.622000] creating model\n",
      "[2016-12-04 23:41:29.648000] adding scores\n",
      "[2016-12-04 23:41:29.648000] fitting\n",
      "[2016-12-04 23:41:39.547000] outputting\n",
      "name = model_20_decor_reg_7, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 100000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+8\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_7')\n",
    "model_20_decor_reg_7 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:41:40.654000] creating model\n",
      "[2016-12-04 23:41:41.836000] adding scores\n",
      "[2016-12-04 23:41:41.836000] fitting\n",
      "[2016-12-04 23:41:53.173000] outputting\n",
      "name = model_20_decor_reg_8, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_reg_8')\n",
    "model_20_decor_reg_8 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# + sparse phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:42:07.662000] creating model\n",
      "[2016-12-04 23:42:08.847000] adding scores\n",
      "[2016-12-04 23:42:08.854000] fitting\n",
      "[2016-12-04 23:42:20.794000] outputting\n",
      "name = model_20_decor_sst_1, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "ss_phi_regularizer, tau = -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_1')\n",
    "model_20_decor_sst_1 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:42:40.307000] creating model\n",
      "[2016-12-04 23:42:41.784000] adding scores\n",
      "[2016-12-04 23:42:41.800000] fitting\n",
      "[2016-12-04 23:42:53.020000] outputting\n",
      "name = model_20_decor_sst_2, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "ss_phi_regularizer, tau = -0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -0.7\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_2')\n",
    "model_20_decor_sst_2 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# показатели ядра стали лучше, размер ядра меньше, темы всё равно смешанные и плохие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:43:09] creating model\n",
      "[2016-12-04 23:43:10.313000] adding scores\n",
      "[2016-12-04 23:43:10.319000] fitting\n",
      "[2016-12-04 23:43:24.441000] outputting\n",
      "name = model_20_decor_sst_3, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "ss_phi_regularizer, tau = -1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -1.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_3')\n",
    "model_20_decor_sst_3 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# вроде темы получше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:43:37.683000] creating model\n",
      "[2016-12-04 23:43:38.940000] adding scores\n",
      "[2016-12-04 23:43:38.948000] fitting\n",
      "[2016-12-04 23:43:50.977000] outputting\n",
      "name = model_20_decor_sst_4, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_4')\n",
    "model_20_decor_sst_4 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# вроде темы тоже норм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:43:59.094000] creating model\n",
      "[2016-12-04 23:44:00.271000] adding scores\n",
      "[2016-12-04 23:44:00.278000] fitting\n",
      "[2016-12-04 23:44:12.617000] outputting\n",
      "name = model_20_decor_sst_5, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "ss_phi_regularizer, tau = -2.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_5')\n",
    "model_20_decor_sst_5 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# темы несколько в одной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## + ss theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:44:19.584000] creating model\n",
      "[2016-12-04 23:44:20.706000] adding scores\n",
      "[2016-12-04 23:44:20.706000] fitting\n",
      "[2016-12-04 23:44:31.742000] outputting\n",
      "name = model_20_decor_sst_ssp_1, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -0.5\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -0.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_ssp_1')\n",
    "model_20_decor_sst_ssp_1 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# темы норм / есть общие темы-мусор, разредим еще тету "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:44:38.448000] creating model\n",
      "[2016-12-04 23:44:39.464000] adding scores\n",
      "[2016-12-04 23:44:39.464000] fitting\n",
      "[2016-12-04 23:44:50.696000] outputting\n",
      "name = model_20_decor_sst_ssp_2, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1.5\n",
      "decorrelator_phi_regularizer, tau = 1000000.0\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_ssp_2')\n",
    "model_20_decor_sst_ssp_2 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:44:58.396000] creating model\n",
      "[2016-12-04 23:44:59.659000] adding scores\n",
      "[2016-12-04 23:44:59.668000] fitting\n",
      "[2016-12-04 23:45:11.667000] outputting\n",
      "name = model_20_decor_sst_ssp_3, n_topics = 20, n_doc_passes = 5, seed_value = 100, n_iterations = 15, n_top_tokens = 15, p_threshold = 0.25\n",
      "ss_theta_regularizer, tau = -1.5\n",
      "decorrelator_phi_regularizer, tau = 10000000.0\n",
      "ss_phi_regularizer, tau = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+7\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_ssp_3')\n",
    "model_20_decor_sst_ssp_3 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-12-04 23:45:14.683000] creating model\n",
      "[2016-12-04 23:45:15.963000] adding scores\n",
      "[2016-12-04 23:45:15.973000] fitting\n"
     ]
    }
   ],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -1.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+7\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_ssp_4')\n",
    "model_20_decor_sst_ssp_4 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cтало хуже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -2.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_ssp_5')\n",
    "model_20_decor_sst_ssp_5 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_model = create_model(current_dictionary=dictionary, n_topics=20, n_doc_passes=5, seed_value=100,\n",
    "                         n_top_tokens=15, p_mass_threshold=0.25)\n",
    "tmp_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='ss_phi_regularizer', class_ids=['ngramm']))\n",
    "tmp_model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='ss_theta_regularizer'))\n",
    "tmp_model.regularizers['ss_phi_regularizer'].tau = -2\n",
    "tmp_model.regularizers['ss_theta_regularizer'].tau = -2.5\n",
    "tmp_model.regularizers['decorrelator_phi_regularizer'].tau = 5*1e+6\n",
    "tmp_model = fit_one_model(tmp_model, _n_iterations=15, _model_name='model_20_decor_sst_ssp_6')\n",
    "model_20_decor_sst_ssp_6 = tmp_model; tmp_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
