---
title: "Статистический анализ данных (задание 4)."
subtitle: "Работа с реальными данными."
author: "Юлия Лукашкина, 417 группа"
output: html_document
---

Известны значения приносимого молока на каждую корову в месяц в период с  января 1962г. по декабрь 1975.

Задание: *Построить прогноз на 1/10 длины ряда с округлением до целого числа сезонных периодов.*


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.height=5.5, fig.width=10}
data <- read.csv("Q:/PSAD/task4/monthly-milk-production-pounds-p.csv")
data <- data[-169,]
colnames(data) <- c("date", "milk")
library(forecast)
library(tseries)
library(lmtest)
library(Hmisc)


data$milk <- as.numeric(data$milk)
data$date <- as.Date(as.yearmon(data$date, format="%Y-%m"))
tSeries <- ts(data = data$milk, start = as.numeric(c(format(data$date[1], "%Y"), format(data$date[1], "%m"))), freq = 12)
xname <- "Monthly milk production: pounds per cow."

plot(tSeries, type="l", ylab=xname, col="red")
grid()
```

Попробуем поделить на число дней в месяце:
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
tSeries_old <- tSeries
tSeries <- tSeries / monthDays(as.Date(time(tSeries))) 
plot(tSeries, type="l", ylab=xname, col="red")
grid()
```
Ряд выглядит более стабильно, будем его исследовать.
По внешнему виду ряда можно сделать следующие выводы:

* Выражена сезонность с периодом в 1 год.
* Максимум количества молока достигается примерно в конце первой половины каждого года (весна-лето).
* Существенных выбросов нет.
* Выражен линейный возрастающий тренд.
* Дисперсия постоянна. 

Оптимальное преобразование Бокса-Кокса и результат его применения:
```{r, echo=FALSE, fig.width=10, fig.height=8}
par(mfrow=c(2,1))
plot(tSeries, ylab="Original series", xlab="", col="red")
grid()

LambdaOpt <- BoxCox.lambda(tSeries)
plot(BoxCox(tSeries, LambdaOpt), ylab="Transformed series", xlab="", col="red")
title(main=toString(round(LambdaOpt, 3)))
grid()
```

Применение оптимального преобразования Бокса-Кокса не вносит существенных изменений в поведение временного ряда, в дальнейшем работать будем только с исходными данными.

Проверим ряд на стационарность (p=`r round(kpss.test(tSeries)$p.value, 4)`, критерий KPSS) --- нестационарен.
Ряд имеет выраженный сезонный профиль, проведем сезонное дифференцирование.

```{r, echo=FALSE, fig.height=5.5, fig.width=10}
tSeries_diff <- diff(tSeries, 12)
plot(tSeries_diff, type="l", col="red")
grid()

library(tseries)
#kpss.test(tSeries)
#kpss.test(tSeries_diff)
```

Получился ещё нестационарный ряд (p=`r round(kpss.test(tSeries_diff)$p.value, 4)`, критерий KPSS). Проведём ещё одно дифференцирование.


```{r, echo=FALSE, fig.height=5.5, fig.width=10}
tSeries_dd <- diff(tSeries_diff)
plot(tSeries_dd, type="l", col="red")
grid()
#kpss.test(tSeries_dd)
```

Получили стационарный ряд (p=`r round(kpss.test(tSeries_dd)$p.value, 4)`, критерий KPSS)! 


## ARIMA
### Ручной подбор модели
Посмотрим на ACF и PACF полученного продифференцированного ряда:

```{r, echo=FALSE, fig.height=5.5, fig.width=10}
tsdisplay(diff(diff(tSeries, 12), 1))
```

Значимый 12 лаг (связанный с сезоном). Будем искать модель в окрестности ARIMA(0,1,1)(0,1,1)$_{12}$.

Модель                     | AICc
-------------------------- | ------------
ARIMA(0,1,1)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(0,1,1), seasonal=c(0,1,1))$aicc`
ARIMA(1,1,1)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(1,1,1), seasonal=c(0,1,1))$aicc`
ARIMA(2,1,1)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,1,1), seasonal=c(0,1,1))$aicc`
ARIMA(3,1,1)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(3,1,1), seasonal=c(0,1,1))$aicc`
ARIMA(4,1,1)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(4,1,1), seasonal=c(0,1,1))$aicc`
ARIMA(0,1,1)(0,1,2)$_{12}$ | `r Arima(tSeries, order=c(0,1,1), seasonal=c(0,1,2))$aicc`
ARIMA(1,1,1)(0,1,2)$_{12}$ | `r Arima(tSeries, order=c(1,1,1), seasonal=c(0,1,2))$aicc`
ARIMA(2,1,1)(0,1,2)$_{12}$ | `r Arima(tSeries, order=c(2,1,1), seasonal=c(0,1,2))$aicc`
ARIMA(3,1,1)(0,1,2)$_{12}$ | `r Arima(tSeries, order=c(3,1,1), seasonal=c(0,1,2))$aicc`
ARIMA(2,1,1)(1,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,1,1), seasonal=c(1,1,1))$aicc`
ARIMA(2,1,1)(1,1,0)$_{12}$ | `r Arima(tSeries, order=c(2,1,1), seasonal=c(1,1,0))$aicc`
ARIMA(2,1,1)(1,1,2)$_{12}$ | `r Arima(tSeries, order=c(2,1,1), seasonal=c(1,1,2))$aicc`
ARIMA(1,1,1)(1,1,1)$_{12}$ | `r Arima(tSeries, order=c(1,1,1), seasonal=c(1,1,1))$aicc`
ARIMA(3,1,1)(1,1,1)$_{12}$ | `r Arima(tSeries, order=c(3,1,1), seasonal=c(1,1,1))$aicc`
ARIMA(2,1,2)(1,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,1,2), seasonal=c(1,1,1))$aicc`

Наилучшая по AIC$_c$ модель — ARIMA(0,1,1)(0,1,1)$_{12}$. Исследуем остатки. Здесь и далее не будем рассматривать первые 12 значений остатков.


```{r, echo=FALSE, fig.height=8, fig.width=10}
fit_arima <- Arima(tSeries, order=c(0,1,1), seasonal=c(0,1,1))
res <- residuals(fit_arima)[-c(1:12)]
res_arima <- res
tsdisplay(res)
```

Достигаемые уровни значимости критерия Льюнга-Бокса для остатков:

```{r, echo=FALSE}
p <- rep(0, 1, frequency(tSeries)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(res, lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="P-value", ylim=c(0,1))
abline(h = 0.05, lty = 2, col = "blue")
```

Q-Q plot и гистограмма для остатков:
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(res)
qqline(res, col="red")
hist(res)
```


Гипотеза           | Критерий      | Результат проверки | Достигаемый уровень значимости
------------------ | ------------- | ------------------ | ------------------------------
Нормальность       | Шапиро-Уилка  | отвергается        | `r shapiro.test(res)$p.value`
Несмещённость      | Уилкоксона    | не отвергается     | `r wilcox.test(res)$p.value`
Стационарность     | KPSS          | не отвергается     | `r kpss.test(res)$p.value`
Гомоскедастичность | Бройша-Пагана | не отвергается     | `r bptest(res ~ c(1:length(res)))$p.value`


## AUTO ARIMA
Построим модель автоматически и сравним результат с построенной выше вручную моделью:
```{r, echo=FALSE, fig.height=8, fig.width=10}
auto.arima(tSeries)
```

Построенная автоматически модель по AIC$_c$ не сильно отличается от построенной ранее. Посмотрим на остатки auto ARIMA. 

```{r, echo=FALSE, fig.height=8, fig.width=10}
fit_auto <- auto.arima(tSeries)#_old
res <- residuals(fit_auto)[-c(1:12)]
res_auto <- res
tsdisplay(res)
```

Достигаемые уровни значимости критерия Льюнга-Бокса для остатков:

```{r, echo=FALSE}
p <- rep(0, 1, frequency(tSeries)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(res, lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="P-value", ylim=c(0,1))
abline(h = 0.05, lty = 2, col = "blue")
```

Q-Q plot и гистограмма для остатков:
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(res)
qqline(res, col="red")
hist(res)
```

Гипотеза           | Критерий      | Результат проверки | Достигаемый уровень значимости
------------------ | ------------- | ------------------ | ------------------------------
Нормальность       | Шапиро-Уилка  | отвергается        | `r shapiro.test(res)$p.value`
Несмещённость      | Уилкоксона    | не отвергается     | `r wilcox.test(res)$p.value`
Стационарность     | KPSS          | не отвергается     | `r kpss.test(res)$p.value`
Гомоскедастичность | Бройша-Пагана | не отвергается     | `r bptest(res ~ c(1:length(res)))$p.value`


## Прогноз ETS
```{r, echo=FALSE}
fit_ets <- ets(tSeries)
print(fit_ets)
res <- residuals(fit_ets)[-c(1:12)]
res_ets <- res
tsdisplay(res)
```

Достигаемые уровни значимости критерия Льюнга-Бокса для них:

```{r, echo=FALSE}
p <- rep(0, 1, frequency(tSeries)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(res, lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="P-value", ylim=c(0,1))
abline(h = 0.05, lty = 2, col = "blue")
```

Q-Q plot и гистограмма для остатков:
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(res)
qqline(res, col="red")
hist(res)
```

Гипотеза           | Критерий      | Результат проверки | Достигаемый уровень значимости
------------------ | ------------- | ------------------ | ------------------------------
Нормальность       | Шапиро-Уилка  | отвергается        | `r shapiro.test(res)$p.value`
Несмещённость      | Уилкоксона    | нe отвергается     | `r wilcox.test(res)$p.value`
Стационарность     | KPSS          | не отвергается     | `r kpss.test(res)$p.value`
Гомоскедастичность | Бройша-Пагана | не отвергается     | `r bptest(res ~ c(1:length(res)))$p.value`

## Сравнение моделей
Используем критерий Диболда-Мариано для попарного сравнения трех моделей: ARIMA auto, ARIMA и ETS.   

```{r, echo=FALSE, fig.height=8, fig.width=10,warning=FALSE}
p <- c(0, 0, 0)
dm.test(res_ets, res_arima)
p[1] <- dm.test(res_ets, res_arima)$p.value
dm.test(res_ets, res_auto)
p[2] <- dm.test(res_ets, res_auto)$p.value
dm.test(res_auto, res_arima)
p[3] <- dm.test(res_auto, res_arima)$p.value
p <- unlist(p)
p <-p.adjust(p, "BH")
print(p)
```

C поправкой на множественность Бонферони можно сделать вывод, что все три построенные модели не отличаются друг от друга.


Выберем модель по наименьшей ошибке на тестовой выборке. Последние 24 отсчета возьмем в качестве тестовой выборки. А остальные отсчеты в качестве тренировочной и перестроим на них полученные модели.

Настроив модели на обучающей выборке, построим предсказания на тестовой:
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
train <- window(tSeries, end=c(1973,12))
test  <- window(tSeries, start=c(1974,1))
test_old  <- window(tSeries_old, start=c(1974,1))

fit_arima_tr <- Arima(train, order=c(0,1,1), seasonal=c(0,1,1))
D <- 24
fc_arima  <- forecast(fit_arima_tr, h=D)* monthDays(as.Date(time(test)))
acc_arima <- accuracy(fc_arima, test)
plot(fc_arima, ylab=xname, xlab="Year")
lines(tSeries_old, col="red")

fit_auto_tr <- Arima(train, order=c(0,1,0), seasonal=c(1,1,1))
D <- 24
fc_auto  <- forecast(fit_auto_tr, h=D)   
acc_auto <- accuracy(fc_auto, test)
plot(fc_auto, ylab=xname, xlab="Year")
lines(tSeries, col="red")

fit_ets_tr <- ets(train)
fc_ets     <- forecast(fit_ets_tr, h=D)
acc_ets    <- accuracy(fc_ets, test)
plot(fc_ets, ylab=xname, xlab="Year")
lines(tSeries, col="red")
```

RMSE для ETS модели `r acc_ets[2,]["RMSE"]`, ARIMA --- `r acc_arima[2,]["RMSE"]`, AUTO ARIMA --- `r acc_auto[2,]["RMSE"]`.
Модель ARIMA, настроенная вручную лучше, прогноз будем строить с её помощью.


Итоговый прогноз выбранной модели (ручной ARIMA на трасформированных данных) на следующие 24 месяца:
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
f <- forecast(fit_arima, h=D, bootstrap=TRUE)
print(f)
plot(f, ylab=xname, xlab="Year", col="red")
```